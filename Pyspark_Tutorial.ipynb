{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_jA4fi716Rqa",
    "outputId": "e1130efb-47c8-4af5-944d-0e37c889f5d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=10666f8a2c9639fa8447d110297ab653b0f15d94dbd6ad2213b9eb2d624a83de\n",
      "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark\n",
      "Successfully installed pyspark-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xPBPmKiM6WLs"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"pyspark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "hYcYLipM7To6",
    "outputId": "ea4376ab-ffd2-4825-b776-0973a9e26167"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
       "      pre.function-repr-contents {\n",
       "        overflow-x: auto;\n",
       "        padding: 8px 12px;\n",
       "        max-height: 500px;\n",
       "      }\n",
       "\n",
       "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
       "        cursor: pointer;\n",
       "        max-height: 100px;\n",
       "      }\n",
       "    </style>\n",
       "    <pre style=\"white-space: initial; background:\n",
       "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
       "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.session.SparkSession</b><br/>def __init__(sparkContext: SparkContext, jsparkSession: Optional[JavaObject]=None, options: Dict[str, Any]={})</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py</a>The entry point to programming Spark with the Dataset and DataFrame API.\n",
       "\n",
       "A SparkSession can be used to create :class:`DataFrame`, register :class:`DataFrame` as\n",
       "tables, execute SQL over tables, cache tables, and read parquet files.\n",
       "To create a :class:`SparkSession`, use the following builder pattern:\n",
       "\n",
       ".. versionchanged:: 3.4.0\n",
       "    Supports Spark Connect.\n",
       "\n",
       ".. autoattribute:: builder\n",
       "   :annotation:\n",
       "\n",
       "Examples\n",
       "--------\n",
       "Create a Spark session.\n",
       "\n",
       "&gt;&gt;&gt; spark = (\n",
       "...     SparkSession.builder\n",
       "...         .master(&quot;local&quot;)\n",
       "...         .appName(&quot;Word Count&quot;)\n",
       "...         .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)\n",
       "...         .getOrCreate()\n",
       "... )\n",
       "\n",
       "Create a Spark session with Spark Connect.\n",
       "\n",
       "&gt;&gt;&gt; spark = (\n",
       "...     SparkSession.builder\n",
       "...         .remote(&quot;sc://localhost&quot;)\n",
       "...         .appName(&quot;Word Count&quot;)\n",
       "...         .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)\n",
       "...         .getOrCreate()\n",
       "... )  # doctest: +SKIP</pre>\n",
       "      <script>\n",
       "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
       "        for (const element of document.querySelectorAll('.filepath')) {\n",
       "          element.style.display = 'block'\n",
       "          element.onclick = (event) => {\n",
       "            event.preventDefault();\n",
       "            event.stopPropagation();\n",
       "            google.colab.files.view(element.textContent, 166);\n",
       "          };\n",
       "        }\n",
       "      }\n",
       "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
       "        element.onclick = (event) => {\n",
       "          event.preventDefault();\n",
       "          event.stopPropagation();\n",
       "          element.classList.toggle('function-repr-contents-collapsed');\n",
       "        };\n",
       "      }\n",
       "      </script>\n",
       "      </div>"
      ],
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVMKOC5vi-5C"
   },
   "source": [
    "### Helo Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yp2LVW_4i2QC"
   },
   "outputs": [],
   "source": [
    "help(spark.createDataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9F4CWKBiNRn"
   },
   "source": [
    "### Read csv data in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1KfhVyCV7nBx"
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(path= 'dbfs:/FileStore/data/Employees1.csv', header=True)\n",
    "df = display(df)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_UDDx9Qf927X"
   },
   "outputs": [],
   "source": [
    "df = spark.read.format('csv').option(key='header', value=True).load(path='dbfs: /FileStore/data/Employees1.csv')\n",
    "display(df)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQghUJIXZpHc"
   },
   "source": [
    "### Read json data in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-bV0hqayL6Q"
   },
   "outputs": [],
   "source": [
    "# way:1\n",
    "df.spark.read.json ('dbfs:/FileStore/data/emps.json', multiline = True)\n",
    "df.spark.read.json (['dbfs:/FileStore/data/emps.json','dbfs:/FileStore/data/emps.json'], multiline = True) # read multiple json files\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MB_Ap1AewTx"
   },
   "source": [
    "### Read Parquet file into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGRaRKzUehcy"
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet('dbfs:/FileStore/data/userdata1.parquet')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNJ-iBFsD0no"
   },
   "source": [
    "### Write dataframe into csv\n",
    "1. How to read wite dataframe into csv files using pyspark\n",
    "2. Option available while saving csv files\n",
    "3. Saving Modes\n",
    "\n",
    "    * append: Append contents of this class DataFrame to existing data.\n",
    "    * overwrite: Overwrite existing data.\n",
    "    * ignore: Silently ignore this operation if data already exists.\n",
    "    * error or errorifexists (default case): Throw an exception if data already exists.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "IvRDrtud-jPd",
    "outputId": "cd47d305-6ddd-4f90-9935-8057c25fbeef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, name: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(1, 'Ashutosh'), (2, 'Varsh')]\n",
    "schema= ['id', 'name']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "df.write.csv(path='dbfs: /tmp/emps', header=True, mode=\"erroe\")\n",
    "\n",
    "display(spark.read.csv(path='dbfs: /tmp/emps', header=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2f9vVE9hdAF1"
   },
   "source": [
    "### Write dataframe into json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SvClG0swaZ5j"
   },
   "outputs": [],
   "source": [
    "data = [(1, 'Ashutosh'), (2, 'Varsh')]\n",
    "schema= ['id', 'name']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "df.write.json(path='dbfs: /tmp/emps', header=True, mode=\"erroe\")\n",
    "\n",
    "display(spark.read.json(path='dbfs: /tmp/emps', header=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qiLFYaBiaGy7"
   },
   "outputs": [],
   "source": [
    "from re import MULTILINE\n",
    "# way: 2\n",
    "df = spark.read.format('org.apache.spark.sql.json').load('dbfs:/FileStore/data/emps.json', multiline = True)\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYtjcikQi6Eg"
   },
   "source": [
    "### Write dataframe into Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJDrK-w8j0Ru"
   },
   "outputs": [],
   "source": [
    "data = [(1, 'Ashutosh'), (2, 'Varsh')]\n",
    "schema= ['id', 'name']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "df.write.parquet(\"/folder_location\", mode='error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZ1HnVU4NK1F"
   },
   "source": [
    "### show() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z78DCpypNJDd",
    "outputId": "ca08d2c5-cc6e-47f4-8ee5-1f0159360288"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------------------------------------------------+\n",
      "|id |commments                                                          |\n",
      "+---+-------------------------------------------------------------------+\n",
      "|1  |dfkdjshjkdbdjfbgdbfgfbdgjkbfdjkgbfskdjbgsfjkdbgjskdbgkjsdbfgjsbdfgb|\n",
      "|2  |dlfnd;sjhfga;dhfgjadfbgjkfdabgadfjbgadrbfguruigbfdubgiufhdg        |\n",
      "|3  |egrfiuegfuiegfuigeifgsdgfuisdgfiugafiuergifugiuafiuagfiugafuigguaig|\n",
      "|4  |jhdfghdjbfgbhjfgbhgbfdgbfdgbbfdghbshgfdhsghfdglshlhshdfgshgdshkj   |\n",
      "+---+-------------------------------------------------------------------+\n",
      "\n",
      "+---+---------+\n",
      "| id|commments|\n",
      "+---+---------+\n",
      "|  1| dfkdj...|\n",
      "|  2| dlfnd...|\n",
      "|  3| egrfi...|\n",
      "|  4| jhdfg...|\n",
      "+---+---------+\n",
      "\n",
      "+---+--------------------+\n",
      "| id|           commments|\n",
      "+---+--------------------+\n",
      "|  1|dfkdjshjkdbdjfbgd...|\n",
      "|  2|dlfnd;sjhfga;dhfg...|\n",
      "+---+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "-RECORD 0-------------------------\n",
      " id        | 1                    \n",
      " commments | dfkdjshjkdbdjfbgd... \n",
      "-RECORD 1-------------------------\n",
      " id        | 2                    \n",
      " commments | dlfnd;sjhfga;dhfg... \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'dfkdjshjkdbdjfbgdbfgfbdgjkbfdjkgbfskdjbgsfjkdbgjskdbgkjsdbfgjsbdfgb'),\n",
    "        (2, 'dlfnd;sjhfga;dhfgjadfbgjkfdabgadfjbgadrbfguruigbfdubgiufhdg'),\n",
    "        (3, 'egrfiuegfuiegfuigeifgsdgfuisdgfiugafiuergifugiuafiuagfiugafuigguaig'),\n",
    "        (4, 'jhdfghdjbfgbhjfgbhgbfdgbfdgbbfdghbshgfdhsghfdglshlhshdfgshgdshkj')]\n",
    "schema= ['id', 'commments']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "df.show(truncate = False)\n",
    "\n",
    "df.show(truncate = 8)\n",
    "\n",
    "df.show(n=2, truncate = True)\n",
    "\n",
    "df.show(n=2, truncate = True, vertical = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XTlzIJbOvG5"
   },
   "source": [
    "### withColumn()\n",
    "\n",
    "*   PySpark withColumn() is a transformation function of DataFrame which is\n",
    "used to change the value, convert the datatype of an existing column, create a new column, and many more\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vU5K38VyO1Xv"
   },
   "outputs": [],
   "source": [
    "data = [(1, 'dfkdjshjkdbdjfbgdbfgfbdgjkbfdjkgbfskdjbgsfjkdbgjskdbgkjsdbfgjsbdfgb'),\n",
    "        (2, 'dlfnd;sjhfga;dhfgjadfbgjkfdabgadfjbgadrbfguruigbfdubgiufhdg'),\n",
    "        (3, 'egrfiuegfuiegfuigeifgsdgfuisdgfiugafiuergifugiuafiuagfiugafuigguaig'),\n",
    "        (4, 'jhdfghdjbfgbhjfgbhgbfdgbfdgbbfdghbshgfdhsghfdglshlhshdfgshgdshkj')]\n",
    "schema= ['id', 'commments']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aqVXq1jhO6aR",
    "outputId": "13b7197a-b69a-4cc3-d8bf-b9fd969c3a57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+\n",
      "| id|    name|salary|\n",
      "+---+--------+------+\n",
      "|  1|Ashutosh|  3000|\n",
      "|  2|Kumbhare|  4000|\n",
      "|  3|   Varsh|  5000|\n",
      "|  4|    Ajay|  6000|\n",
      "+---+--------+------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+---+--------+------+\n",
      "| id|    name|salary|\n",
      "+---+--------+------+\n",
      "|  1|Ashutosh|  6000|\n",
      "|  2|Kumbhare|  8000|\n",
      "|  3|   Varsh| 10000|\n",
      "|  4|    Ajay| 12000|\n",
      "+---+--------+------+\n",
      "\n",
      "+---+--------+------+-------+\n",
      "| id|    name|salary|country|\n",
      "+---+--------+------+-------+\n",
      "|  1|Ashutosh|  6000|  India|\n",
      "|  2|Kumbhare|  8000|  India|\n",
      "|  3|   Varsh| 10000|  India|\n",
      "|  4|    Ajay| 12000|  India|\n",
      "+---+--------+------+-------+\n",
      "\n",
      "+---+--------+------+-------+-------------+\n",
      "| id|    name|salary|country|copyed_salary|\n",
      "+---+--------+------+-------+-------------+\n",
      "|  1|Ashutosh|  6000|  India|         6000|\n",
      "|  2|Kumbhare|  8000|  India|         8000|\n",
      "|  3|   Varsh| 10000|  India|        10000|\n",
      "|  4|    Ajay| 12000|  India|        12000|\n",
      "+---+--------+------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "data = [(1, 'Ashutosh', 3000),\n",
    "        (2, 'Kumbhare', 4000),\n",
    "        (3, 'Varsh', 5000),\n",
    "        (4, 'Ajay', 6000)]\n",
    "schema= ['id', 'name', 'salary']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "df = df.withColumn('salary', F.col(\"salary\").cast('Integer'))\n",
    "\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "df = df.withColumn('salary', F.col(\"salary\") * 2)\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn('country', F.lit('India'))\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn('copyed_salary', F.col(\"salary\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOwLOf3DTxof"
   },
   "source": [
    "### withColumnRenamed()\n",
    "\n",
    "*   PySpark withColumnRenamed() is a transformation function of DataFrame which is used to change existing column name in dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_6VRXVrhQUW8",
    "outputId": "8d9bddfb-79bc-471d-9632-235df1d4dbb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+\n",
      "| id|    name|Income|\n",
      "+---+--------+------+\n",
      "|  1|Ashutosh|  3000|\n",
      "|  2|Kumbhare|  4000|\n",
      "|  3|   Varsh|  5000|\n",
      "|  4|    Ajay|  6000|\n",
      "+---+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'Ashutosh', 3000),\n",
    "        (2, 'Kumbhare', 4000),\n",
    "        (3, 'Varsh', 5000),\n",
    "        (4, 'Ajay', 6000)]\n",
    "schema= ['id', 'name', 'salary']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "df = df.withColumnRenamed('salary', 'Income')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnKCBoOJUcrF"
   },
   "source": [
    "### StructType() & StructField()\n",
    "\n",
    "*   PySpark StructType & StructField classes are used to programmatically specify the schema to the DataFrame and create complex columns like nested struct, array, and map columns.\n",
    "\n",
    "*   StructType is a collection of StructField's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K5s1nQNgURB0",
    "outputId": "5decfde9-31fd-4ec5-cf6e-218da4eca603"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+\n",
      "| id|    name|salary|\n",
      "+---+--------+------+\n",
      "|  1|Ashutosh|  3000|\n",
      "|  2|Kumbhare|  4000|\n",
      "|  3|   Varsh|  5000|\n",
      "|  4|    Ajay|  6000|\n",
      "+---+--------+------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "data = [(1, 'Ashutosh', 3000),\n",
    "        (2, 'Kumbhare', 4000),\n",
    "        (3, 'Varsh', 5000),\n",
    "        (4, 'Ajay', 6000)]\n",
    "\n",
    "schema = T.StructType([\n",
    "                      T.StructField(name='id', dataType=T.IntegerType()),\n",
    "                      T.StructField(name='name', dataType=T.StringType()),\n",
    "                      T.StructField(name='salary', dataType=T.IntegerType())\n",
    "                      ])\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "erpZ7WMjW342",
    "outputId": "7faf273a-db43-4628-de9b-96a173461642"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------+\n",
      "| id|                name|salary|\n",
      "+---+--------------------+------+\n",
      "|  1|{Ashutosh, Kumbhare}|  3000|\n",
      "|  2|         {Ashu, abc}|  4000|\n",
      "|  3|        {Varsh, cde}|  5000|\n",
      "|  4|         {Ajay, efg}|  6000|\n",
      "+---+--------------------+------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- First_name: string (nullable = true)\n",
      " |    |-- Last_name: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "data = [(1, ('Ashutosh','Kumbhare'), 3000),\n",
    "        (2, ('Ashu','abc'), 4000),\n",
    "        (3, ('Varsh','cde'), 5000),\n",
    "        (4, ('Ajay','efg'), 6000)]\n",
    "\n",
    "my_structType = T.StructType(\n",
    "        [\n",
    "        T.StructField(name='First_name', dataType=T.StringType()),\n",
    "        T.StructField(name='Last_name', dataType=T.StringType())\n",
    "        ]\n",
    "    )\n",
    "\n",
    "schema = T.StructType(\n",
    "                      [\n",
    "                      T.StructField(name='id', dataType=T.IntegerType()),\n",
    "                      T.StructField(name='name', dataType=my_structType),\n",
    "                      T.StructField(name='salary', dataType=T.IntegerType())\n",
    "                      ]\n",
    "        )\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTbouiKmboQ9"
   },
   "source": [
    "### ArrayType Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mywqGzpRdMvI",
    "outputId": "1906b889-b46a-4b6f-9f43-903e86c72244"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|numbers|\n",
      "+---+-------+\n",
      "|abc| [1, 2]|\n",
      "|mno| [4, 5]|\n",
      "|xyz| [7, 8]|\n",
      "+---+-------+\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- numbers: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n",
      "+---+-------+----------+\n",
      "| id|numbers|index_zero|\n",
      "+---+-------+----------+\n",
      "|abc| [1, 2]|         1|\n",
      "|mno| [4, 5]|         4|\n",
      "|xyz| [7, 8]|         7|\n",
      "+---+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "data = [('abc', [1,2]), ('mno', [4,5]), ('xyz', [7,8])]\n",
    "\n",
    "schema = T.StructType([T.StructField(name='id', dataType= T.StringType()),\n",
    "                       T.StructField(name='numbers', dataType= T.ArrayType(T.IntegerType()))\n",
    "                      ])\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "df = df.withColumn('index_zero', F.col('numbers')[0])\n",
    "# df = df.withColumn('index_zero', df.numbers[0])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LPpQHphDgFKD",
    "outputId": "d4b90832-4c4e-4f2a-9fbd-a6cfdc42034e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+----------+\n",
      "| id|    name|salary|new_column|\n",
      "+---+--------+------+----------+\n",
      "|  1|Ashutosh|  3000| [1, 3000]|\n",
      "|  2|Kumbhare|  4000| [2, 4000]|\n",
      "|  3|   Varsh|  5000| [3, 5000]|\n",
      "|  4|    Ajay|  6000| [4, 6000]|\n",
      "+---+--------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'Ashutosh', 3000),\n",
    "        (2, 'Kumbhare', 4000),\n",
    "        (3, 'Varsh', 5000),\n",
    "        (4, 'Ajay', 6000)]\n",
    "\n",
    "schema = T.StructType([\n",
    "                      T.StructField(name='id', dataType=T.IntegerType()),\n",
    "                      T.StructField(name='name', dataType=T.StringType()),\n",
    "                      T.StructField(name='salary', dataType=T.IntegerType())\n",
    "                      ])\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "df = df.withColumn('new_column',\n",
    "                   F.array(\n",
    "                       F.col('id'),\n",
    "                       F.col('salary')\n",
    "                       )\n",
    "                   )\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaaDm2_VbwD1"
   },
   "source": [
    "### explode(), split(), array() & array_contains() Functions\n",
    "\n",
    "*   Use explode() function to create a new row for each element in the given array column.\n",
    "\n",
    "*   split() sql function returns an array type after splitting the string column by delimiter.\n",
    "\n",
    "*   Use array() function to create a new array column by merging the data from multiple columns.\n",
    "\n",
    "*   array_contains() sql function is used to check if array column contains a value. Returns null if the array is null, true if the array contains the value, and false otherwise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfHQDXLd0pt1"
   },
   "source": [
    "#### 1. explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gI7aVhH6ZRnI",
    "outputId": "50e91730-acd9-48d1-88e9-7c8eff9c5fff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+\n",
      "| id|numbers|explodedCol|\n",
      "+---+-------+-----------+\n",
      "|abc| [1, 2]|          1|\n",
      "|abc| [1, 2]|          2|\n",
      "|efg| [4, 5]|          4|\n",
      "|efg| [4, 5]|          5|\n",
      "|xyz| [7, 8]|          7|\n",
      "|xyz| [7, 8]|          8|\n",
      "+---+-------+-----------+\n",
      "\n",
      "+---+-----------+\n",
      "| id|explodedCol|\n",
      "+---+-----------+\n",
      "|abc|          1|\n",
      "|abc|          2|\n",
      "|abc|          1|\n",
      "|abc|          2|\n",
      "|efg|          4|\n",
      "|efg|          5|\n",
      "|efg|          4|\n",
      "|efg|          5|\n",
      "|xyz|          7|\n",
      "|xyz|          8|\n",
      "|xyz|          7|\n",
      "|xyz|          8|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "data = [('abc', [1,2]), ('efg', [4,5]), ('xyz', [7,8])]\n",
    "\n",
    "schema = T.StructType(\n",
    "    [\n",
    "        T.StructField('id', T.StringType()),\n",
    "        T.StructField('numbers', T.ArrayType(T.IntegerType()))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "df = spark.createDataFrame(data,schema)\n",
    "\n",
    "df = df.withColumn('explodedCol', F.explode(F.col('numbers')))\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn('explodedCol', F.explode(F.col('numbers')))\\\n",
    "     .select(\n",
    "          'id',\n",
    "          'explodedCol'\n",
    "          )\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIUPbhVp6ZJr"
   },
   "source": [
    "#### 2. split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r99KCopZ0hpE",
    "outputId": "5e9244b6-5550-4160-d073-b477fc566e1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------------------+\n",
      "| id|    name|            skills|\n",
      "+---+--------+------------------+\n",
      "|  1|Ashutosh|dothet, azure, sql|\n",
      "|  2|   Varsh|    java, aws, sql|\n",
      "+---+--------+------------------+\n",
      "\n",
      "+---+--------+--------------------+\n",
      "| id|    name|              skills|\n",
      "+---+--------+--------------------+\n",
      "|  1|Ashutosh|[dothet,  azure, ...|\n",
      "|  2|   Varsh|  [java,  aws,  sql]|\n",
      "+---+--------+--------------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- skills: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "data = [(1, 'Ashutosh', 'dothet, azure, sql'), (2, 'Varsh','java, aws, sql')]\n",
    "\n",
    "schema = ['id', 'name', 'skills']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn('skills',F.split('skills',','))\n",
    "df.show()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTYuwjGNDYfx"
   },
   "source": [
    "#### 3. array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86mNCKSr7X2P",
    "outputId": "b8bc4e15-58e9-45c9-aa6f-b444c46ba51e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------------+--------------+\n",
      "| id|    name|primarySkill|secondarySkill|\n",
      "+---+--------+------------+--------------+\n",
      "|  1|Ashutosh|      dotnet|         azure|\n",
      "|  2|  Varsha|        java|           aws|\n",
      "+---+--------+------------+--------------+\n",
      "\n",
      "+---+--------+------------+--------------+---------------+\n",
      "| id|    name|primarySkill|secondarySkill|         skills|\n",
      "+---+--------+------------+--------------+---------------+\n",
      "|  1|Ashutosh|      dotnet|         azure|[dotnet, azure]|\n",
      "|  2|  Varsha|        java|           aws|    [java, aws]|\n",
      "+---+--------+------------+--------------+---------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- primarySkill: string (nullable = true)\n",
      " |-- secondarySkill: string (nullable = true)\n",
      " |-- skills: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "data = [(1, 'Ashutosh', 'dotnet', 'azure'), (2, 'Varsha', 'java', 'aws')]\n",
    "\n",
    "schema = ['id', 'name', 'primarySkill', 'secondarySkill']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn('skills', F.array(F.col('primarySkill'), F.col('secondarySkill')))\n",
    "df.show()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYkhf1CyEmD9"
   },
   "source": [
    "#### 4. array_contains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GZSdMFv3ErVM",
    "outputId": "778a0c97-d810-45ce-adc2-d7e3e66776be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+-------------+\n",
      "| id|    name|       skills|Has JavaSkill|\n",
      "+---+--------+-------------+-------------+\n",
      "|  1|   Varsh|[.net, azure]|        false|\n",
      "|  2|Ashutosh|  [java, aws]|         true|\n",
      "+---+--------+-------------+-------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- skills: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- Has JavaSkill: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "data = [(1, 'Varsh', ['.net', 'azure']), (2, 'Ashutosh', ['java', 'aws'])]\n",
    "\n",
    "schema = ['id', 'name', 'skills']\n",
    "\n",
    "df = spark.createDataFrame (data, schema)\n",
    "\n",
    "df = df.withColumn ('Has JavaSkill', F.array_contains(F.col('skills'), 'java'))\n",
    "df.show()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bw1pe4L3FCfu"
   },
   "source": [
    "### MapType column\n",
    "\n",
    "*  PySpark MapType is used to represent map key-value pair similar to python Dictionary (Dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uos_xHvpFHub",
    "outputId": "13f61acc-988a-407b-d7d6-dd1b50480dd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|    name|          properties|\n",
      "+--------+--------------------+\n",
      "|Ashutosh|{eye -> brown, ha...|\n",
      "|   varsh|{eye -> blue, hai...|\n",
      "+--------+--------------------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('Ashutosh', {'hair': 'black', 'eye': 'brown'}), ('varsh', { 'hair': 'black', 'eye': 'blue'})]\n",
    "\n",
    "schema = ['name', 'properties']\n",
    "\n",
    "df = spark.createDataFrame (data, schema)\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IHK_yMJkGo5H",
    "outputId": "47880d20-7792-4f9a-cc88-d72603457257"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------------+\n",
      "|name    |properties                   |\n",
      "+--------+-----------------------------+\n",
      "|Ashutosh|{eye -> brown, hair -> black}|\n",
      "|varsh   |{eye -> blue, hair -> black} |\n",
      "+--------+-----------------------------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "data = [('Ashutosh', { 'hair': 'black', 'eye': 'brown'}), ('varsh', {'hair': 'black', 'eye': 'blue'})]\n",
    "\n",
    "schema = T.StructType(\n",
    "    [\n",
    "        T.StructField('name', T.StringType()),\n",
    "        T.StructField('properties', T.MapType(T.StringType(), T.StringType()))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUxJbpbSIEmH"
   },
   "source": [
    "#### Access MapType elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CRkxXvD_HQ4m",
    "outputId": "2cb79dc4-b392-47f3-8977-e580f4ba5bb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------------+-----+\n",
      "|name    |properties                   |hair |\n",
      "+--------+-----------------------------+-----+\n",
      "|Ashutosh|{eye -> brown, hair -> black}|black|\n",
      "|varsh   |{eye -> blue, hair -> black} |black|\n",
      "+--------+-----------------------------+-----+\n",
      "\n",
      "+--------+-----------------------------+-----+\n",
      "|name    |properties                   |eye  |\n",
      "+--------+-----------------------------+-----+\n",
      "|Ashutosh|{eye -> brown, hair -> black}|brown|\n",
      "|varsh   |{eye -> blue, hair -> black} |blue |\n",
      "+--------+-----------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# way 1\n",
    "\n",
    "df1 = df.withColumn('hair', df.properties['hair'])\n",
    "\n",
    "df1.show(truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "# way2\n",
    "\n",
    "df2 = df.withColumn('eye', df.properties.getItem('eye'))\n",
    "\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QayXagz7NeHD"
   },
   "source": [
    "### map_keys(), map_values(), explode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3zhPR1vRGer"
   },
   "source": [
    "### explode() with MapType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7hyPoy8RP0z",
    "outputId": "ede13250-ce6f-497c-ebe3-4da45fa05b0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------------+\n",
      "|name    |properties                   |\n",
      "+--------+-----------------------------+\n",
      "|Ashutosh|{eye -> brown, hair -> black}|\n",
      "|varsh   |{eye -> blue, hair -> black} |\n",
      "+--------+-----------------------------+\n",
      "\n",
      "+--------+-----------------------------+----+-----+\n",
      "|name    |properties                   |key |value|\n",
      "+--------+-----------------------------+----+-----+\n",
      "|Ashutosh|{eye -> brown, hair -> black}|eye |brown|\n",
      "|Ashutosh|{eye -> brown, hair -> black}|hair|black|\n",
      "|varsh   |{eye -> blue, hair -> black} |eye |blue |\n",
      "|varsh   |{eye -> blue, hair -> black} |hair|black|\n",
      "+--------+-----------------------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "data = [('Ashutosh', { 'hair': 'black', 'eye': 'brown'}), ('varsh', {'hair': 'black', 'eye': 'blue'})]\n",
    "\n",
    "schema = T.StructType(\n",
    "    [\n",
    "        T.StructField('name', T.StringType()),\n",
    "        T.StructField('properties', T.MapType(T.StringType(), T.StringType()))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show(truncate=False)\n",
    "\n",
    "df1 = df.select('name', 'properties', F.explode(df.properties))\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRfX00-LOBDS"
   },
   "source": [
    "#### 2. map_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1wgNNXahMfG6",
    "outputId": "ec5211b3-9685-45ef-d532-c02dc813dee9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------------+-----------+\n",
      "|name    |properties                   |keys       |\n",
      "+--------+-----------------------------+-----------+\n",
      "|Ashutosh|{eye -> brown, hair -> black}|[eye, hair]|\n",
      "|varsh   |{eye -> blue, hair -> black} |[eye, hair]|\n",
      "+--------+-----------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn('keys', F.map_keys(df.properties))\n",
    "\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mz-pPSptWkjp"
   },
   "source": [
    "#### 3. map.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hzJaP5S1T2PT",
    "outputId": "6ef07c50-8e87-4ec7-93e9-0646bde88bd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------------+--------------+\n",
      "|name    |properties                   |keys          |\n",
      "+--------+-----------------------------+--------------+\n",
      "|Ashutosh|{eye -> brown, hair -> black}|[brown, black]|\n",
      "|varsh   |{eye -> blue, hair -> black} |[blue, black] |\n",
      "+--------+-----------------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn('keys', F.map_values(df.properties))\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOGoSUDQXHTO"
   },
   "source": [
    "### Row() class\n",
    "\n",
    "*   pyspark.sql.Row which is represented as a record/row in DataFrame, one can create a Row object by using named arguments or create a custom Row like class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "THmCf8hPW5Js",
    "outputId": "efd3de0a-ff18-4abe-9db7-2f081c8fa453"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ashutosh\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "row = Row(name = 'Ashutosh', salary = 2000)\n",
    "\n",
    "print(row.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o_7LixEhcmIJ",
    "outputId": "9593d85d-cee9-4514-a42d-92b5e43d0e3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|    name|salary|\n",
      "+--------+------+\n",
      "|Ashutosh|  2000|\n",
      "|  Keahav|  3000|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row1 = Row(name = 'Ashutosh', salary = 2000)\n",
    "row2 = Row(name = 'Keahav', salary = 3000)\n",
    "\n",
    "data = [row1, row2]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LcHDbcdxhRCn"
   },
   "source": [
    "#### create nested struct type using Row()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iWKHwiqhgsGh",
    "outputId": "02c54f36-9981-42c7-e897-d710fb42fbc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- prop: struct (nullable = true)\n",
      " |    |-- hair: string (nullable = true)\n",
      " |    |-- eye: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "data=[Row(name=\"keashav\", prop=Row(hair=\"black\", eye=\"blue\")), Row(name=\"Ashutosh\", prop=Row(hair=\"grey\", eye=\"black\"))]\n",
    "df=spark.createDataFrame(data)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnFPJHsYGf26"
   },
   "source": [
    "### Column() Class\n",
    "\n",
    "*   PySpark Column class represents a single Column in a DataFrame. pyspark.sql.Column class provides several functions to work with DataFrame to manipulate the Column values, evaluate the boolean expression to filter rows, retrieve a value or part of a value from a DataFrame column\n",
    "\n",
    "*  One of the simplest ways to create a Column class object is by using PySpark lit() SQL function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yBnX-xMAHFw4",
    "outputId": "aeae086b-3876-47a7-effc-486dfb708c2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.column.Column'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "col1 = F.lit(\"zero\")\n",
    "\n",
    "print(type(col1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EeZPSAbuImqs"
   },
   "source": [
    "#### Access columns in multiple ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rO_DBUIaHlqd",
    "outputId": "ba7db77b-ead0-49f5-c4cf-03096b02ea74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|salary|\n",
      "+------+\n",
      "|  3000|\n",
      "|  4000|\n",
      "|  5000|\n",
      "|  6000|\n",
      "+------+\n",
      "\n",
      "+------+\n",
      "|salary|\n",
      "+------+\n",
      "|  3000|\n",
      "|  4000|\n",
      "|  5000|\n",
      "|  6000|\n",
      "+------+\n",
      "\n",
      "+------+\n",
      "|salary|\n",
      "+------+\n",
      "|  3000|\n",
      "|  4000|\n",
      "|  5000|\n",
      "|  6000|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "data = [(1, 'Ashutosh', 3000),\n",
    "        (2, 'Kumbhare', 4000),\n",
    "        (3, 'Varsh', 5000),\n",
    "        (4, 'Ajay', 6000)]\n",
    "schema= ['id', 'name', 'salary']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "df.select(df.salary).show()\n",
    "df.select(df['salary']).show()\n",
    "df.select(F.col('salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37Dd904-IyQM",
    "outputId": "e0eafa08-0822-4097-d4f0-284389d0d13b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|name.First_name|\n",
      "+---------------+\n",
      "|       Ashutosh|\n",
      "|           Ashu|\n",
      "|          Varsh|\n",
      "|           Ajay|\n",
      "+---------------+\n",
      "\n",
      "+----------+\n",
      "|First_name|\n",
      "+----------+\n",
      "|  Ashutosh|\n",
      "|      Ashu|\n",
      "|     Varsh|\n",
      "|      Ajay|\n",
      "+----------+\n",
      "\n",
      "+----------+\n",
      "|First_name|\n",
      "+----------+\n",
      "|  Ashutosh|\n",
      "|      Ashu|\n",
      "|     Varsh|\n",
      "|      Ajay|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "data = [(1, ('Ashutosh','Kumbhare'), 3000),\n",
    "        (2, ('Ashu','abc'), 4000),\n",
    "        (3, ('Varsh','cde'), 5000),\n",
    "        (4, ('Ajay','efg'), 6000)]\n",
    "\n",
    "my_structType = T.StructType(\n",
    "        [\n",
    "        T.StructField(name='First_name', dataType=T.StringType()),\n",
    "        T.StructField(name='Last_name', dataType=T.StringType())\n",
    "        ]\n",
    "    )\n",
    "\n",
    "schema = T.StructType(\n",
    "                      [\n",
    "                      T.StructField(name='id', dataType=T.IntegerType()),\n",
    "                      T.StructField(name='name', dataType=my_structType),\n",
    "                      T.StructField(name='salary', dataType=T.IntegerType())\n",
    "                      ]\n",
    "        )\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "df.select(df.name.First_name).show()\n",
    "df.select(df['name.First_name']).show()\n",
    "df.select(F.col('name.First_name')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPoPAGI4Mhi9"
   },
   "source": [
    "### when() & otherwise()\n",
    "\n",
    "*   It is similar to SQL Case When, executes sequence of expressions until it\n",
    "matches the condition and returns a value when match.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5vufDAlFLfQe",
    "outputId": "cb34f6e3-0a41-43d5-8887-462b7c174fb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+------+\n",
      "| id|   name|gender|salary|\n",
      "+---+-------+------+------+\n",
      "|  1|   Ashu|     M|  2000|\n",
      "|  2| Varsha|     F|  3000|\n",
      "|  3|Bhushan|      |  2000|\n",
      "+---+-------+------+------+\n",
      "\n",
      "+---+-------+-------+\n",
      "| id|   name| gender|\n",
      "+---+-------+-------+\n",
      "|  1|   Ashu|   Male|\n",
      "|  2| Varsha| FeMale|\n",
      "|  3|Bhushan|unknown|\n",
      "+---+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "data = [(1, 'Ashu', 'M', 2000), (2, 'Varsha', 'F', 3000), (3, 'Bhushan', '', 2000)]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'salary']\n",
    "\n",
    "df = spark.createDataFrame (data, schema)\n",
    "\n",
    "df.show()\n",
    "\n",
    "df1 = df.select(\n",
    "    df.id,\n",
    "    df.name,\n",
    "    F.when(condition=df.gender == 'M', value='Male')\n",
    "    .when(condition=df.gender == 'F',value='FeMale')\n",
    "    .otherwise ('unknown').alias('gender')\n",
    "    )\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIV0FQQyPZh-"
   },
   "source": [
    "### alias(), asc(), desc(), cast() & like()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3ITqIH5PdKm"
   },
   "source": [
    "#### 1. alies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "krvKdYA-OcWo",
    "outputId": "a870c04a-c3d8-4faf-ac93-5c9b48929cb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  1|  Ashu|     M|  2000|\n",
      "|  2|Keshav|     M|  4000|\n",
      "|  3|Varsha|     F|  3000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'Ashu', 'M',2000), (2, 'Keshav', 'M', 4000), (3, 'Varsha', 'F', 3000)]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'salary']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.select(\n",
    "    df.id.alias('emp_id'),\n",
    "    df.name.alias ('emp_name')\n",
    "    )\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2ZFXGYgQJPb"
   },
   "source": [
    "### asc(), desc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ap7aHiaoQAdc",
    "outputId": "0edb3de1-2839-4e91-f5da-923cdb16162b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+------+\n",
      "| id|    name|gender|salary|\n",
      "+---+--------+------+------+\n",
      "|  3|  Sonali|     F|  3000|\n",
      "|  2| Bhushan|     M|  4000|\n",
      "|  1|Ashutosh|     M|  2000|\n",
      "+---+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'Ashutosh', 'M', 2000), (2, 'keshav', 'M', 4000), (3, 'Bhushan', 'F', 3000)]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'salary']\n",
    "\n",
    "spark.createDataFrame(data, schema)\n",
    "\n",
    "df.sort(df.name.desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGjg7Nx4wewZ"
   },
   "source": [
    "#### cast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FmvzOWnTQwmq",
    "outputId": "6353e6ad-1a56-4b03-cdc1-6c4b8cb2c9f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'Ashutosh', 'M', 2000), (2, 'Bhushan', 'M', 4000), (3, 'Sonali', 'F', 3000)]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'salary']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.printSchema()\n",
    "\n",
    "df1 = df.select(df.salary.cast('int'))\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ija4Ly3-w_AI"
   },
   "source": [
    "#### like()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iFqOI24QwrsK",
    "outputId": "c1e33a02-b955-43f8-e5c4-827b643ca1a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+------+\n",
      "| id|name|gender|salary|\n",
      "+---+----+------+------+\n",
      "+---+----+------+------+\n",
      "\n",
      "+---+--------+------+------+\n",
      "| id|    name|gender|salary|\n",
      "+---+--------+------+------+\n",
      "|  1|Ashutosh|     M|  2000|\n",
      "+---+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'Ashutosh', 'M', 2000), (2, 'Keshav', 'M', 4000), (3, 'Varsha', 'F', 3000)]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'salary']\n",
    "\n",
    "df = spark.createDataFrame (data, schema)\n",
    "\n",
    "df.filter(df.name.like ('a%')).show()\n",
    "\n",
    "# Like functiuon is case sensitive.\n",
    "df.filter(df.name.like ('A%')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHArIe94ySxd"
   },
   "source": [
    "### filter()\n",
    "\n",
    "* PySpark filter() function is used to filter the rows from DataFrame based on the given condition or SQL expression.\n",
    "\n",
    "### where()\n",
    "\n",
    "* You can also use where() clause instead of the filter() if you are coming from an SQL background, both these functions operate exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3FscKSElxOsH",
    "outputId": "07f1992a-4d70-487a-e72e-509c10a37c3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+------+\n",
      "| id|    name|gender|salary|\n",
      "+---+--------+------+------+\n",
      "|  1|Ashutosh|     M|  2000|\n",
      "|  2|  Keshav|     M|  4000|\n",
      "+---+--------+------+------+\n",
      "\n",
      "+---+--------+------+------+\n",
      "| id|    name|gender|salary|\n",
      "+---+--------+------+------+\n",
      "|  1|Ashutosh|     M|  2000|\n",
      "|  2|  Keshav|     M|  4000|\n",
      "+---+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'Ashutosh', 'M', 2000), (2, 'Keshav', 'M', 4000), (3, 'Varsha', 'F',3000)]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'salary']\n",
    "\n",
    "df = spark.createDataFrame (data, schema)\n",
    "\n",
    "# where function\n",
    "df.where(df.gender == 'M').show()\n",
    "\n",
    "# filter function\n",
    "df.filter(df.gender =='M').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsKv30JI2FGD"
   },
   "source": [
    "###  distinct()\n",
    "* PySpark distinct() function is used to remove the duplicate rows (all columns)\n",
    "\n",
    "### dropDuplicates()\n",
    "* dropDuplicates() is used to drop rows based on selected (one or multiple) columns.\n",
    "\n",
    "So basically, using these functions we can get distinct rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1khEtZxG2GUT",
    "outputId": "4af2edf8-ed83-49d7-8601-42102fe5feea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+------+\n",
      "| id|    name|gender|salary|\n",
      "+---+--------+------+------+\n",
      "|  2|  Keshav|     M|  4000|\n",
      "|  1|Ashutosh|     M|  2000|\n",
      "|  2|   Akash|     M|  4000|\n",
      "|  3|  Varsha|     F|  3000|\n",
      "+---+--------+------+------+\n",
      "\n",
      "+---+--------+------+------+\n",
      "| id|    name|gender|salary|\n",
      "+---+--------+------+------+\n",
      "|  2|  Keshav|     M|  4000|\n",
      "|  1|Ashutosh|     M|  2000|\n",
      "|  2|   Akash|     M|  4000|\n",
      "|  3|  Varsha|     F|  3000|\n",
      "+---+--------+------+------+\n",
      "\n",
      "+---+--------+------+------+\n",
      "| id|    name|gender|salary|\n",
      "+---+--------+------+------+\n",
      "|  3|  Varsha|     F|  3000|\n",
      "|  1|Ashutosh|     M|  2000|\n",
      "+---+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'Ashutosh', 'M', 2000), (2, 'Keshav', 'M', 4000), (2, 'Akash', 'M',4000), (3, 'Varsha', 'F', 3000)]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'salary']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.distinct().show()\n",
    "\n",
    "df.dropDuplicates ().show()\n",
    "\n",
    "df.dropDuplicates (['gender']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04C9jjp32nSD"
   },
   "source": [
    "### orderBy() & sort()\n",
    "\n",
    "* You can use either sort() or orderBy() function of PySpark DataFrame to sort DataFrame by ascending or descending order based on single or multiple columns.\n",
    "\n",
    "* By default, sorting will happen in ascending order. We can explicitly mention ascending or descending using asc(), desc() functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZXauVOoj3ljp",
    "outputId": "fc9a42f6-cdda-406a-c7ff-a81fcc9b2f1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+------+-------+\n",
      "| id|    name|gender|salary|    dep|\n",
      "+---+--------+------+------+-------+\n",
      "|  4|   Akash|     M|  3000|     HR|\n",
      "|  2|  Keshav|     M|  4000|     HR|\n",
      "|  1|Ashutosh|     M|  2000|     IT|\n",
      "|  3|  Varsha|     F|  3000|Payroll|\n",
      "+---+--------+------+------+-------+\n",
      "\n",
      "+---+--------+------+------+-------+\n",
      "| id|    name|gender|salary|    dep|\n",
      "+---+--------+------+------+-------+\n",
      "|  3|  Varsha|     F|  3000|Payroll|\n",
      "|  1|Ashutosh|     M|  2000|     IT|\n",
      "|  2|  Keshav|     M|  4000|     HR|\n",
      "|  4|   Akash|     M|  3000|     HR|\n",
      "+---+--------+------+------+-------+\n",
      "\n",
      "+---+--------+------+------+-------+\n",
      "| id|    name|gender|salary|    dep|\n",
      "+---+--------+------+------+-------+\n",
      "|  3|  Varsha|     F|  3000|Payroll|\n",
      "|  1|Ashutosh|     M|  2000|     IT|\n",
      "|  4|   Akash|     M|  3000|     HR|\n",
      "|  2|  Keshav|     M|  4000|     HR|\n",
      "+---+--------+------+------+-------+\n",
      "\n",
      "+---+--------+------+------+-------+\n",
      "| id|    name|gender|salary|    dep|\n",
      "+---+--------+------+------+-------+\n",
      "|  4|   Akash|     M|  3000|     HR|\n",
      "|  2|  Keshav|     M|  4000|     HR|\n",
      "|  1|Ashutosh|     M|  2000|     IT|\n",
      "|  3|  Varsha|     F|  3000|Payroll|\n",
      "+---+--------+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'Ashutosh', 'M',2000, 'IT'), (2, 'Keshav', 'M',4000, 'HR'), (3, 'Varsha', 'F',3000, 'Payroll'), (4, 'Akash', 'M',3000, 'HR')]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'salary', 'dep']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# sort function\n",
    "df.sort('dep', 'salary').show()\n",
    "df.sort(df.dep.desc(),df.salary.desc()).show()\n",
    "\n",
    "# orderBy\n",
    "df.orderBy(df.dep.desc(),df.salary.asc()).show()\n",
    "df.orderBy('dep', 'salary').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6VDOE4I7jKo"
   },
   "source": [
    "###  union() & unionAll()\n",
    "\n",
    "* union() and unionAll() transformations are used to merge two or more DataFrame's of the same schema or structure.\n",
    "\n",
    "* union() & unionAll() method merges two DataFrames and returns the new DataFrame with all rows from two Dataframes regardless of duplicate data. To remove duplicates use distinct() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_6rvQgQI4ABf",
    "outputId": "5eba3f61-f936-4ef1-b518-9ef724523eba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+------+-------+\n",
      "| id|    name|gender|salary|    dep|\n",
      "+---+--------+------+------+-------+\n",
      "|  1|Ashutosh|     M|  2000|     IT|\n",
      "|  2|  Keshav|     M|  4000|     HR|\n",
      "|  3|  Varsha|     F|  3000|Payroll|\n",
      "+---+--------+------+------+-------+\n",
      "\n",
      "+---+--------+------+------+-------+\n",
      "| id|    name|gender|salary|    dep|\n",
      "+---+--------+------+------+-------+\n",
      "|  1|Ashutosh|     M|  2000|     IT|\n",
      "|  2|  Keshav|     M|  4000|     HR|\n",
      "|  3|  Varsha|     F|  3000|Payroll|\n",
      "+---+--------+------+------+-------+\n",
      "\n",
      "+---+--------+------+------+-------+\n",
      "| id|    name|gender|salary|    dep|\n",
      "+---+--------+------+------+-------+\n",
      "|  1|Ashutosh|     M|  2000|     IT|\n",
      "|  2|  Keshav|     M|  4000|     HR|\n",
      "|  3|  Varsha|     F|  3000|Payroll|\n",
      "|  1|Ashutosh|     M|  2000|     IT|\n",
      "|  2|  Keshav|     M|  4000|     HR|\n",
      "|  3|  Varsha|     F|  3000|Payroll|\n",
      "+---+--------+------+------+-------+\n",
      "\n",
      "+---+--------+------+------+-------+\n",
      "| id|    name|gender|salary|    dep|\n",
      "+---+--------+------+------+-------+\n",
      "|  1|Ashutosh|     M|  2000|     IT|\n",
      "|  2|  Keshav|     M|  4000|     HR|\n",
      "|  3|  Varsha|     F|  3000|Payroll|\n",
      "|  1|Ashutosh|     M|  2000|     IT|\n",
      "|  2|  Keshav|     M|  4000|     HR|\n",
      "|  3|  Varsha|     F|  3000|Payroll|\n",
      "+---+--------+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'Ashutosh', 'M', 2000, 'IT'), (2, 'Keshav', 'M',4000, 'HR'), (3, 'Varsha', 'F', 3000, 'Payroll')]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'salary', 'dep']\n",
    "\n",
    "\n",
    "df1 = spark.createDataFrame(data, schema)\n",
    "\n",
    "df2 = spark.createDataFrame(data, schema)\n",
    "\n",
    "df1.show()\n",
    "\n",
    "df2.show()\n",
    "\n",
    "df1.unionAll(df2).show()\n",
    "\n",
    "df1.union(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DD8UWkJH9UsM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ORkWKJU_ZTf"
   },
   "source": [
    "### groupBy()\n",
    "\n",
    "* Similar to SQL GROUP BY clause, PySpark groupBy() function is used to collect the identical data into groups on DataFrame and perform count, sum, avg, min, max functions on the grouped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iJWNqkP__gQV",
    "outputId": "6c9449b4-f25d-426a-a9bf-c9f786d7c6af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|    dep|count|\n",
      "+-------+-----+\n",
      "|Payroll|    2|\n",
      "|     IT|    3|\n",
      "|     HR|    2|\n",
      "+-------+-----+\n",
      "\n",
      "+-------+------+-----+\n",
      "|    dep|gender|count|\n",
      "+-------+------+-----+\n",
      "|Payroll|     F|    1|\n",
      "|     IT|     M|    2|\n",
      "|Payroll|     M|    1|\n",
      "|     HR|     M|    1|\n",
      "|     HR|     F|    1|\n",
      "|     IT|     F|    1|\n",
      "+-------+------+-----+\n",
      "\n",
      "+-------+-----------+\n",
      "|    dep|min(salary)|\n",
      "+-------+-----------+\n",
      "|Payroll|       2000|\n",
      "|     IT|       3000|\n",
      "|     HR|       2000|\n",
      "+-------+-----------+\n",
      "\n",
      "+-------+-----------+\n",
      "|    dep|max(salary)|\n",
      "+-------+-----------+\n",
      "|Payroll|       2500|\n",
      "|     IT|       6000|\n",
      "|     HR|       4000|\n",
      "+-------+-----------+\n",
      "\n",
      "+-------+-----------------+\n",
      "|    dep|      avg(salary)|\n",
      "+-------+-----------------+\n",
      "|Payroll|           2250.0|\n",
      "|     IT|4666.666666666667|\n",
      "|     HR|           3000.0|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data= [\n",
    "    (1, 'Ashutosh', 'M', 5000, 'IT'),\n",
    "     (2, 'Keshav', 'M', 6000, 'IT'),\n",
    "      (3, 'Varsha', 'F',2500, 'Payroll'),\n",
    "       (4, 'Bhushan', 'M',4000, 'HR'),\n",
    "        (5, 'Jiya', 'F', 2000, 'HR'),\n",
    "         (6, 'Akash', 'M', 2000, 'Payroll'),\n",
    "          (7, 'ayesha', 'F',3000, 'IT')\n",
    "          ]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'salary', 'dep']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.groupBy('dep').count().show()\n",
    "\n",
    "df.groupBy('dep', 'gender').count().show()\n",
    "\n",
    "df.groupBy('dep').min('salary').show()\n",
    "\n",
    "df.groupBy('dep').max('salary').show()\n",
    "\n",
    "df.groupBy('dep').avg('salary').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XK-pWtZnBqyi"
   },
   "source": [
    "### groupBy().agg()\n",
    "\n",
    "\n",
    "*   PySpark Groupby agg() is used to calculate more than one aggregate (multiple aggregates) at a time on grouped Data Frame\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MqtKsMKFABh2",
    "outputId": "67929794-b4ba-4833-b425-07c62ba05a78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|    dep|count|\n",
      "+-------+-----+\n",
      "|Payroll|    2|\n",
      "|     IT|    3|\n",
      "|     HR|    2|\n",
      "+-------+-----+\n",
      "\n",
      "+-------+-----------+------+------+\n",
      "|    dep|countOfEmps|minSal|maxSal|\n",
      "+-------+-----------+------+------+\n",
      "|Payroll|          2|  2000|  2500|\n",
      "|     IT|          3|  3000|  6000|\n",
      "|     HR|          2|  2000|  4000|\n",
      "+-------+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "data= [(1, 'Ashutosh', 'M', 5000, 'IT'), (2, 'Keshav', 'M',6000, 'IT'),(3, 'Varsha', 'F',2500, 'Payroll'), (4, 'Bhushan', 'M', 4000, 'HR'), (5, 'Jiya', 'F', 2000, 'HR'), (6, 'Akash', 'M', 2000, 'Payroll'),\n",
    "\n",
    "      (7, 'ayesha', 'F',3000, 'IT')]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'salary', 'dep']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.groupBy('dep').count().show()\n",
    "\n",
    "\n",
    "\n",
    "df.groupBy('dep').agg(F.count('*').alias('countOfEmps'), F.min('salary').alias ('minSal'), F.max ('salary').alias ('maxSal')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrfkhKj3DSee"
   },
   "source": [
    "### unionByName()\n",
    "* unionByName() lets you to merge/union two DataFrames with a different number of columns (different schema) by passing allowMissingColumns with the value true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uVDvuIFdDT9O",
    "outputId": "fe089761-7757-40ab-c351-f30b4ced1a27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+\n",
      "| id|    name|gender|\n",
      "+---+--------+------+\n",
      "|  1|Ashutosh|  male|\n",
      "|  1|  Keshav|  2000|\n",
      "+---+--------+------+\n",
      "\n",
      "+---+--------+------+------+\n",
      "| id|    name|gender|salary|\n",
      "+---+--------+------+------+\n",
      "|  1|Ashutosh|  male|  NULL|\n",
      "|  1|  Keshav|  NULL|  2000|\n",
      "+---+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datal = [(1, 'Ashutosh', 'male')]\n",
    "schema1 = ['id', 'name', 'gender']\n",
    "\n",
    "data2 = [(1, 'Keshav', 2000)]\n",
    "schema2 = ['id', 'name', 'salary']\n",
    "\n",
    "df1 = spark.createDataFrame(datal, schema1)\n",
    "df2 = spark.createDataFrame (data2, schema2)\n",
    "\n",
    "df1.union(df2).show()\n",
    "\n",
    "df1.unionByName(allowMissingColumns=True, other=df2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFQysVDsEAzF"
   },
   "source": [
    "### select()\n",
    "\n",
    "* select() function is used to select single, multiple, column by index, all columns from the list and the nested columns from a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ToC__6S2EJBk",
    "outputId": "08693bd6-d1eb-4c54-8300-87f1edf47825"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|    name|\n",
      "+---+--------+\n",
      "|  1|Ashutosh|\n",
      "|  2|  Keshav|\n",
      "|  3|  Varsha|\n",
      "+---+--------+\n",
      "\n",
      "+---+--------+\n",
      "| id|    name|\n",
      "+---+--------+\n",
      "|  1|Ashutosh|\n",
      "|  2|  Keshav|\n",
      "|  3|  Varsha|\n",
      "+---+--------+\n",
      "\n",
      "+---+--------+\n",
      "| id|    name|\n",
      "+---+--------+\n",
      "|  1|Ashutosh|\n",
      "|  2|  Keshav|\n",
      "|  3|  Varsha|\n",
      "+---+--------+\n",
      "\n",
      "+---+--------+\n",
      "| id|    name|\n",
      "+---+--------+\n",
      "|  1|Ashutosh|\n",
      "|  2|  Keshav|\n",
      "|  3|  Varsha|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'Ashutosh', 'male', 2000), (2, 'Keshav', 'male', 3000), (3, 'Varsha', 'female', 2500)]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'salary']\n",
    "\n",
    "df = spark.createDataFrame (data, schema)\n",
    "\n",
    "#select single or multiple columns df.select('id', 'name').show()\n",
    "df.select(df.id,df.name).show()\n",
    "df.select(df['id'],df['name']).show()\n",
    "\n",
    "#using col() function\n",
    "from pyspark.sql. functions import col\n",
    "df.select(col('id'), col('name')).show()\n",
    "df.select(['id', 'name']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hU173yzcE64j"
   },
   "source": [
    "### join()\n",
    "\n",
    "* join() is like SQL JOIN. We can combine columns from different DataFrames based on condition. It supports all basic join types such as INNER, LEFT,OUTER, RIGHT OUTER, LEFT ANTI, LEFT SEMI, CROSS, SELF\n",
    "\n",
    "*  leftsemi join similar to inner join but get columns only from left dataframe for matching rows.\n",
    "\n",
    "*  leftanti opposite to leftsemi, it gets not matching rows from left dataframe. Self join, joins data with same dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m5JCWFgWE8ny",
    "outputId": "6189c4aa-0d04-44bb-df1d-c231be620dd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+---+\n",
      "| id|    name|salary|dep|\n",
      "+---+--------+------+---+\n",
      "|  1|Ashutosh|  2000|  2|\n",
      "|  2|  Keshav|  3000|  1|\n",
      "|  3| Bhushan|  1000|  4|\n",
      "+---+--------+------+---+\n",
      "\n",
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  1|     IT|\n",
      "|  2|     HR|\n",
      "|  3|Payroll|\n",
      "+---+-------+\n",
      "\n",
      "+---+--------+------+---+---+----+\n",
      "| id|    name|salary|dep| id|name|\n",
      "+---+--------+------+---+---+----+\n",
      "|  2|  Keshav|  3000|  1|  1|  IT|\n",
      "|  1|Ashutosh|  2000|  2|  2|  HR|\n",
      "+---+--------+------+---+---+----+\n",
      "\n",
      "+---+--------+------+---+----+----+\n",
      "| id|    name|salary|dep|  id|name|\n",
      "+---+--------+------+---+----+----+\n",
      "|  1|Ashutosh|  2000|  2|   2|  HR|\n",
      "|  2|  Keshav|  3000|  1|   1|  IT|\n",
      "|  3| Bhushan|  1000|  4|NULL|NULL|\n",
      "+---+--------+------+---+----+----+\n",
      "\n",
      "+----+--------+------+----+---+-------+\n",
      "|  id|    name|salary| dep| id|   name|\n",
      "+----+--------+------+----+---+-------+\n",
      "|   2|  Keshav|  3000|   1|  1|     IT|\n",
      "|NULL|    NULL|  NULL|NULL|  3|Payroll|\n",
      "|   1|Ashutosh|  2000|   2|  2|     HR|\n",
      "+----+--------+------+----+---+-------+\n",
      "\n",
      "+----+--------+------+----+----+-------+\n",
      "|  id|    name|salary| dep|  id|   name|\n",
      "+----+--------+------+----+----+-------+\n",
      "|   2|  Keshav|  3000|   1|   1|     IT|\n",
      "|   1|Ashutosh|  2000|   2|   2|     HR|\n",
      "|NULL|    NULL|  NULL|NULL|   3|Payroll|\n",
      "|   3| Bhushan|  1000|   4|NULL|   NULL|\n",
      "+----+--------+------+----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datal = [(1, 'Ashutosh',2000,2), (2, 'Keshav',3000,1), (3, 'Bhushan', 1000,4)]\n",
    "schema1 = ['id', 'name', 'salary', 'dep']\n",
    "empDf = spark.createDataFrame(datal, schema1)\n",
    "\n",
    "data2 = [(1, 'IT'), (2, 'HR'), (3, 'Payroll')]\n",
    "schema2 = ['id', 'name']\n",
    "depDf = spark.createDataFrame (data2, schema2)\n",
    "\n",
    "empDf.show()\n",
    "depDf.show()\n",
    "\n",
    "empDf.join(depDf, empDf.dep==depDf.id, 'inner').show()\n",
    "empDf.join(depDf, empDf.dep==depDf.id, 'left').show()\n",
    "empDf.join(depDf, empDf.dep==depDf.id, 'right').show()\n",
    "empDf.join(depDf, empDf.dep==depDf.id, 'full').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6ct7CTNHud2"
   },
   "source": [
    "### pivot() function\n",
    "\n",
    "* It's used to rotate data in one column into multiple columns.\n",
    "\n",
    "* It is an aggregation where one of the grouping column values will be converted in individual columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MA7JSv2ZH0GV",
    "outputId": "71b818f4-485e-4bb6-8892-1abcb65c6c46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+---+\n",
      "| id|    name|gender|dep|\n",
      "+---+--------+------+---+\n",
      "|  1|Ashutosh|  male| IT|\n",
      "|  2|  Keshav|  male| IT|\n",
      "|  3|  Varsha|female| HR|\n",
      "|  4|    Jiya|female| IT|\n",
      "|  5|  shakti|female| IT|\n",
      "|  6| Bhushan|  male| HR|\n",
      "|  7|   akash|  male| HR|\n",
      "|  8|  ayesha|female| IT|\n",
      "+---+--------+------+---+\n",
      "\n",
      "+---+------+----+\n",
      "|dep|female|male|\n",
      "+---+------+----+\n",
      "| HR|     1|   2|\n",
      "| IT|     3|   2|\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'Ashutosh', 'male', 'IT'),\n",
    "        (2, 'Keshav', 'male', 'IT'),\n",
    "        (3, 'Varsha', 'female', 'HR'),\n",
    "        (4, 'Jiya', 'female', 'IT'),\n",
    "        (5, 'shakti', 'female', 'IT'),\n",
    "        (6, 'Bhushan', 'male', 'HR'),\n",
    "        (7, 'akash', 'male', 'HR'),\n",
    "        (8, 'ayesha', 'female', 'IT')\n",
    "]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'dep']\n",
    "\n",
    "df = spark.createDataFrame (data, schema)\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.groupBy('dep').pivot('gender').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqWNasjRIbFD"
   },
   "source": [
    "### unpivot Dataframe\n",
    "\n",
    "* Unpivot is rotating columns into rows. PySpark SQL doesn't have unpivot function hence will use the stack() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uM2ul8qXIc1C",
    "outputId": "ca7a77c2-0ab8-4c54-ada0-cfbd493b34a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+\n",
      "|    dep|male|female|\n",
      "+-------+----+------+\n",
      "|     IT|   8|     5|\n",
      "|Payroll|   3|     2|\n",
      "|     HR|   2|     4|\n",
      "+-------+----+------+\n",
      "\n",
      "+-------+------+-----+\n",
      "|    dep|gender|count|\n",
      "+-------+------+-----+\n",
      "|     IT|  Male|    8|\n",
      "|     IT|Female|    5|\n",
      "|Payroll|  Male|    3|\n",
      "|Payroll|Female|    2|\n",
      "|     HR|  Male|    2|\n",
      "|     HR|Female|    4|\n",
      "+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "data = [('IT', 8, 5),('Payroll', 3, 2),('HR', 2, 4)]\n",
    "\n",
    "schema = ['dep', 'male', 'female']\n",
    "\n",
    "df = spark.createDataFrame (data, schema)\n",
    "df.show()\n",
    "\n",
    "\n",
    "df.select('dep', F.expr(\"stack (2, 'Male', male, 'Female', female) as (gender, count)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k36NyHh5M2EH"
   },
   "source": [
    "### fill() & fillna()\n",
    "\n",
    "*\n",
    "fillna() or DataFrameNaFunctions.fill() is used to replace NULL/None values\n",
    "on all or selected multiple DataFrame columns with either zero(0), empty string, space, or any constant literal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gKsWVQ04M_j9",
    "outputId": "60fee8ba-da70-47e3-b2ab-58c381bb01d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+------+----+\n",
      "| id|    name|gender|salary| dep|\n",
      "+---+--------+------+------+----+\n",
      "|  1|Ashutosh|  male|  1000|NULL|\n",
      "|  2|  Varsha|Female|  2000|  IT|\n",
      "|  3|  Keshav|  NULL|  1000|  HR|\n",
      "+---+--------+------+------+----+\n",
      "\n",
      "+---+--------+--------+------+----+\n",
      "| id|    name|  gender|salary| dep|\n",
      "+---+--------+--------+------+----+\n",
      "|  1|Ashutosh|    male|  1000|NULL|\n",
      "|  2|  Varsha|  Female|  2000|  IT|\n",
      "|  3|  Keshav|No_value|  1000|  HR|\n",
      "+---+--------+--------+------+----+\n",
      "\n",
      "+---+--------+------+------+--------+\n",
      "| id|    name|gender|salary|     dep|\n",
      "+---+--------+------+------+--------+\n",
      "|  1|Ashutosh|  male|  1000|No_value|\n",
      "|  2|  Varsha|Female|  2000|      IT|\n",
      "|  3|  Keshav|  NULL|  1000|      HR|\n",
      "+---+--------+------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'Ashutosh', 'male', 1000, None), (2, 'Varsha', 'Female', 2000, 'IT'), (3, 'Keshav', None, 1000, 'HR')]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'salary', 'dep']\n",
    "\n",
    "df = spark.createDataFrame (data, schema)\n",
    "df.show()\n",
    "\n",
    "df.na.fill('No_value', ['gender']).show()\n",
    "df.fillna('No_value', ['dep']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfqKlnFcNsvO"
   },
   "source": [
    "### Sample()\n",
    "\n",
    "* To get the random sampling subset from the large dataset\n",
    "\n",
    "* Use fraction to indicate what percentage of data to return and seed value to make sure every time to get same random sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmuXZkdKNuVM"
   },
   "outputs": [],
   "source": [
    "# Assume df as big data\n",
    "\n",
    "df = spark.range(start=1, end=101)\n",
    "\n",
    "# fraction prameter takes a numeric value\n",
    "# fraction = 0.1 --> 10% of data\n",
    "df1 = df.sample(fraction=0.1, seed=123)\n",
    "\n",
    "df2 = df.sample(fraction=0.1, seed=123)\n",
    "\n",
    "display(df1)\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InKrFgrCQSu4"
   },
   "source": [
    "### collect()\n",
    "* collect() retrieves all elements in a DataFrame as an Array of Row type to the driver node.\n",
    "\n",
    "* collect() is an action hence it does not return a DataFrame instead, it returns data in an Array to the driver. Once the data is in an array, you can use python for loop to process it further.\n",
    "\n",
    "* collect() use it with small DataFrames. With big DataFrames it may result in out of memory error as its return entier data to single node(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OGxsr5F9Q8gO",
    "outputId": "25cc47a8-d3f6-4360-a1e7-fc6b55885657"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+------+----+\n",
      "| id|    name|gender|salary| dep|\n",
      "+---+--------+------+------+----+\n",
      "|  1|Ashutosh|  male|  1000|NULL|\n",
      "|  2|  Varsha|Female|  2000|  IT|\n",
      "|  3| Bhushan|  NULL|  1000|  HR|\n",
      "+---+--------+------+------+----+\n",
      "\n",
      "[Row(id=1, name='Ashutosh', gender='male', salary=1000, dep=None), Row(id=2, name='Varsha', gender='Female', salary=2000, dep='IT'), Row(id=3, name='Bhushan', gender=None, salary=1000, dep='HR')]\n",
      "Row(id=1, name='Ashutosh', gender='male', salary=1000, dep=None)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'Ashutosh', 'male', 1000, None), (2, 'Varsha', 'Female', 2000, 'IT'), (3, 'Bhushan', None, 1000, 'HR')]\n",
    "schema = ['id', 'name', 'gender', 'salary', 'dep']\n",
    "\n",
    "df = spark.createDataFrame (data, schema)\n",
    "df.show()\n",
    "\n",
    "dataRows = df.collect()\n",
    "print(dataRows)\n",
    "\n",
    "print(dataRows[0])\n",
    "\n",
    "print(dataRows[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHaHYjOQmC69"
   },
   "source": [
    "### DataFrame.transform()\n",
    "\n",
    "* It's is used to chain the custom transformations and this function returns the new DataFrame after applying the specified transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0rAPiDrJmCX-",
    "outputId": "e935acdc-ca96-44db-bbf8-40a631a1a14d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+\n",
      "| id|    name|salary|\n",
      "+---+--------+------+\n",
      "|  1|Ashutosh|  2000|\n",
      "|  2| Bhushan|  3000|\n",
      "+---+--------+------+\n",
      "\n",
      "+---+--------+------+\n",
      "| id|    name|salary|\n",
      "+---+--------+------+\n",
      "|  1|ASHUTOSH|  4000|\n",
      "|  2| BHUSHAN|  6000|\n",
      "+---+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "data = [(1, 'Ashutosh', 2000), (2, 'Bhushan', 3000)]\n",
    "schema = ['id', 'name', 'salary']\n",
    "\n",
    "df = spark.createDataFrame (data, schema)\n",
    "df.show()\n",
    "\n",
    "\n",
    "def convertToUpper (df):\n",
    "  return df.withColumn('name', F.upper (df.name))\n",
    "\n",
    "def doubleTheSalary (df):\n",
    "  return df.withColumn('salary', df.salary * 2)\n",
    "\n",
    "df1 = df.transform(convertToUpper).transform(doubleTheSalary)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eh3D_800ys6l"
   },
   "source": [
    "### pyspark.sql.functions.transform()\n",
    "\n",
    "It's is used to apply the transformation on a column of type Array. This function applies the specified transformation on every element of the array and returns an object of ArrayType."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tY-xBewqy0T6",
    "outputId": "6ff2cee6-8f36-4966-edd8-751acda5415c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------+\n",
      "| id|    name|        skills|\n",
      "+---+--------+--------------+\n",
      "|  1|Ashutosh|[azure, dotnt]|\n",
      "|  2|  Keshav|   [aws, java]|\n",
      "+---+--------+--------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- skills: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+---+--------+--------------+\n",
      "| id|    name|        skills|\n",
      "+---+--------+--------------+\n",
      "|  1|Ashutosh|[AZURE, DOTNT]|\n",
      "|  2|  Keshav|   [AWS, JAVA]|\n",
      "+---+--------+--------------+\n",
      "\n",
      "+--------------------------------------------------------------------------------------+\n",
      "|transform(skills, lambdafunction(upper(namedlambdavariable()), namedlambdavariable()))|\n",
      "+--------------------------------------------------------------------------------------+\n",
      "|                                                                        [AZURE, DOTNT]|\n",
      "|                                                                           [AWS, JAVA]|\n",
      "+--------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "data = [(1, 'Ashutosh', ['azure', 'dotnt']), (2, 'Keshav', ['aws', 'java'])]\n",
    "schema = ['id', 'name','skills']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "df.select('id', 'name', F.transform('skills', lambda x: F.upper (x)).alias('skills')).show()\n",
    "\n",
    "def convertToUpper1(x):\n",
    "  return F.upper(x)\n",
    "\n",
    "df.select(F.transform('skills', convertToUpper1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZV-n9fb09C2S"
   },
   "source": [
    "### createOrReplaceTempView()\n",
    "\n",
    "* Advantage of Spark, you can work with SQL along with DataFrames. That means, if you are comfortable with SQL, you can create temporary view on Dataframe by using createOrReplace TempView() and use SQL to select and manipulate data.\n",
    "\n",
    "* Temp Views are session scoped and cannot be shared between the sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WMgv-D119Sfl",
    "outputId": "791fcf1a-b8b7-44e9-ccb0-5bf5e2aa7fff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+\n",
      "| id|    name|salary|\n",
      "+---+--------+------+\n",
      "|  1|Ashutosh|  2000|\n",
      "|  2| Bhushan|  3000|\n",
      "+---+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'Ashutosh', 2000), (2, 'Bhushan',3000)]\n",
    "\n",
    "schema = ['id', 'name', 'salary']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# will create a temp table called employees\n",
    "df.createOrReplaceTempView('employees')\n",
    "\n",
    "df1 = spark.sql('SELECT * FROM employees')\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BW2lYeDDVrWk",
    "outputId": "4d3f7b0b-ce6b-49f5-fcc7-b3e93204b2c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+\n",
      "| id|    name|salary|\n",
      "+---+--------+------+\n",
      "|  1|Ashutosh|  2000|\n",
      "|  2| Bhushan|  3000|\n",
      "+---+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Note: Instade of using spark.sql() function we can normally use sql queries by using magic function\n",
    "\n",
    "\n",
    "data = [(1, 'Ashutosh', 2000), (2, 'Bhushan',3000)]\n",
    "\n",
    "schema = ['id', 'name', 'salary']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# will create a temp table called employees\n",
    "df.createOrReplaceTempView('employees')\n",
    "\n",
    "df1 = spark.sql('SELECT * FROM employees')\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCy4R6SFQVmg"
   },
   "source": [
    "### UDF(user defined function)\n",
    "\n",
    "*   These are similar to functions in SQL. We define some logic in functions and store them in Database and use them in queries.\n",
    "\n",
    "*   Similar to that we can write our own custom logic in python function and register it with PySpark using udf() function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t1StXxq9_JHC",
    "outputId": "37dfd0da-3d3f-4c3d-95ba-11401046fee4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+-----+\n",
      "| id|    name|salary|bonus|\n",
      "+---+--------+------+-----+\n",
      "|  1|Ashutosh|  2000|  500|\n",
      "|  2|  Keshav|  4000| 1000|\n",
      "+---+--------+------+-----+\n",
      "\n",
      "+---+--------+------+-----+--------+\n",
      "| id|    name|salary|bonus|totalPay|\n",
      "+---+--------+------+-----+--------+\n",
      "|  1|Ashutosh|  2000|  500|    2500|\n",
      "|  2|  Keshav|  4000| 1000|    5000|\n",
      "+---+--------+------+-----+--------+\n",
      "\n",
      "+---+--------+------+-----+--------+\n",
      "| id|    name|salary|bonus|totalPay|\n",
      "+---+--------+------+-----+--------+\n",
      "|  1|Ashutosh|  2000|  500|    2500|\n",
      "|  2|  Keshav|  4000| 1000|    5000|\n",
      "+---+--------+------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Way 1\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "\n",
    "data = [(1, 'Ashutosh', 2000, 500), (2, 'Keshav', 4000, 1000)]\n",
    "\n",
    "schema = ['id', 'name', 'salary', 'bonus']\n",
    "\n",
    "df = spark.createDataFrame (data, schema)\n",
    "df.show()\n",
    "\n",
    "def totalPay (s,b):\n",
    "  return s+b\n",
    "\n",
    "# registering a udf\n",
    "TotalPay = F.udf(lambda x,y: totalPay(x,y), T.IntegerType()) # --> F.udf(lambda {parameter_1}, {parameter_2}: udf_name(parameter_1,parameter_2), {udf_return_Type()})\n",
    "\n",
    "df.select('*', TotalPay (F.col('salary'), F.col('bonus')).alias('totalPay')).show()\n",
    "\n",
    "# calling udf\n",
    "df.withColumn('totalPay', TotalPay(df. salary, df.bonus)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tUB3PyzuR98F",
    "outputId": "699b5ca7-969e-4e89-9d7b-2f6d6f63a20b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+-----+\n",
      "| id|    name|salary|bonus|\n",
      "+---+--------+------+-----+\n",
      "|  1|Ashutosh|  2000|  500|\n",
      "|  2|  Varsha|  4000| 1000|\n",
      "+---+--------+------+-----+\n",
      "\n",
      "+---+--------+------+-----+--------+\n",
      "| id|    name|salary|bonus|totalPay|\n",
      "+---+--------+------+-----+--------+\n",
      "|  1|Ashutosh|  2000|  500|    2500|\n",
      "|  2|  Varsha|  4000| 1000|    5000|\n",
      "+---+--------+------+-----+--------+\n",
      "\n",
      "+---+--------+------+-----+--------+\n",
      "| id|    name|salary|bonus|totalPay|\n",
      "+---+--------+------+-----+--------+\n",
      "|  1|Ashutosh|  2000|  500|    2500|\n",
      "|  2|  Varsha|  4000| 1000|    5000|\n",
      "+---+--------+------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# way 2\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "data =[(1, 'Ashutosh', 2000, 500), (2, 'Varsha', 4000, 1000)]\n",
    "schema = ['id', 'name', 'salary', 'bonus']\n",
    "\n",
    "df = spark.createDataFrame (data, schema)\n",
    "df.show()\n",
    "\n",
    "@udf (returnType = T.IntegerType())\n",
    "def totalPay(s,b):\n",
    "  return s+b\n",
    "\n",
    "# calling udf\n",
    "df.select('*', totalPay(F.col('salary'),F.col('bonus')).alias('totalPay')).show()\n",
    "\n",
    "df.withColumn('totalPay', totalPay(df.salary, df.bonus)).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UFJV2jv7pqQy",
    "outputId": "81ad8c16-9533-44c2-a7c2-fc6bd845409b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+-----+--------------------------+\n",
      "| id|    name|salary|bonus|TotalPaySQL(salary, bonus)|\n",
      "+---+--------+------+-----+--------------------------+\n",
      "|  1|Ashutosh|  2000|  500|                      2500|\n",
      "|  2|   Vasha|  4000| 1000|                      5000|\n",
      "+---+--------+------+-----+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# way 3 using udf with createOrReplaceTempView()\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "\n",
    "data = [(1, 'Ashutosh', 2000, 500), (2, 'Vasha',4000, 1000)]\n",
    "\n",
    "schema = ['id', 'name', 'salary', 'bonus']\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# udf & createOrReplaceTempView()\n",
    "\n",
    "# step 1: creating temporary view on Dataframe\n",
    "df.createOrReplaceTempView('emps')\n",
    "\n",
    "# step 2: create python function\n",
    "def totalPay (s,b):\n",
    "  return s+b\n",
    "\n",
    "# step 3: to register udf function so that we can use in SQL queries\n",
    "spark.udf.register(name='TotalPaySQL', f=totalPay, returnType=T.IntegerType())\n",
    "\n",
    "# step 4: using udf in SQL querie\n",
    "df = spark.sql('SELECT *, TotalPaySQL(salary, bonus) FROM emps')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7gT_GqJNvVs"
   },
   "source": [
    "### Convert RDD(Resilient Distributed Dataset) to Dataframe\n",
    "\n",
    "\n",
    "* Its collection of objects similar to list in Python. Its Immutable and In memory processing.\n",
    "* By using parallelize() function of SparkContext you can create an RDD.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LLDiY5S8jE45",
    "outputId": "6e7dcd8d-8ca6-408d-9541-e87a552ef643"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'Ashutosh'), (2, 'Varsha')]\n",
      "+---+--------+\n",
      "| id|    name|\n",
      "+---+--------+\n",
      "|  1|Ashutosh|\n",
      "|  2|  Varsha|\n",
      "+---+--------+\n",
      "\n",
      "+---+--------+\n",
      "| id|    name|\n",
      "+---+--------+\n",
      "|  1|Ashutosh|\n",
      "|  2|  Varsha|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'Ashutosh'), (2, 'Varsha')]\n",
    "\n",
    "# creating rdd\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "print(rdd.collect())\n",
    "\n",
    "\n",
    "#converting rdd to dataframe\n",
    "\n",
    "# way 1\n",
    "df = rdd.toDF(schema=['id', 'name'])\n",
    "df.show()\n",
    "\n",
    "# way 2\n",
    "df1 = spark.createDataFrame(rdd, ['id', 'name'])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aLCUkeEQQoc"
   },
   "source": [
    "### map() transformation\n",
    "\n",
    "*   ts RDD transformation used to apply function(lambda) on every element of RDD and returns new RDD.\n",
    "*    Dataframe doesn't have map() transformation to use with Dataframe you need to generate RDD first.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Nb1OY1EQQbl",
    "outputId": "6fd978c7-7680-444c-b88a-81c3de0f1591"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ashu', 'kumbhare', 'Ashu kumbhare'), ('Varsha', 'Kumbhare', 'Varsha Kumbhare')]\n"
     ]
    }
   ],
   "source": [
    "data = [('Ashu', 'kumbhare'), ('Varsha', 'Kumbhare')]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "rdd1 = rdd.map(lambda x: x + (x[0]+' '+x[1],))\n",
    "print(rdd1.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuxpYHK0YI6h"
   },
   "source": [
    "###  flatMap() transformation\n",
    "\n",
    "*   flatMap() is a transformation operation that flattens the RDD (array/map DataFrame columns) after applying the function on every element and returns a new PySpark RDD\n",
    "\n",
    "*   Its not available in dataframes. Explode() functions can be used in dataframes to flatten arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "29mcm7qDXQdr",
    "outputId": "332d9f30-9a96-47aa-f0e5-1516235af0c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ashu kumbhare', 'Varsha Kumbhare']\n",
      "Ashu kumbhare\n",
      "Varsha Kumbhare\n",
      "Ashu\n",
      "kumbhare\n",
      "Varsha\n",
      "Kumbhare\n"
     ]
    }
   ],
   "source": [
    "data =['Ashu kumbhare', 'Varsha Kumbhare']\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "print(rdd.collect())\n",
    "\n",
    "for item in rdd.collect():\n",
    "  print(item)\n",
    "\n",
    "rdd1=rdd.flatMap(lambda x: x.split(' '))\n",
    "\n",
    "for item in rdd1.collect():\n",
    "  print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJRRSDBTceM-"
   },
   "source": [
    "### partitionBy function\n",
    "\n",
    "*   Its used to partition large Dataset into smaller files based on one or multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3j8t5M7MYvvA"
   },
   "outputs": [],
   "source": [
    "data = [(1, 'Ashutosh', 'male', 'IT'), (2, 'keshav', 'male', 'HR'), (3, 'varsha', 'female', 'IT')]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'dep']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.write.parquet ('path', mode= 'overwrite', partitionBy= 'dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaVjScN5e_ww"
   },
   "source": [
    "### from_json() function\n",
    "\n",
    "*  • It's used to convert json string in to MapType or StructType. In this video we discuss about converting it to MapType.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "c2sSkU0efJmX",
    "outputId": "fd9b83c7-9787-4dab-f425-b52c6484f784"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------------+\n",
      "|name    |props                           |\n",
      "+--------+--------------------------------+\n",
      "|Ashutosh|{\"hair\": \"black\",\"eye\": \"brown\"}|\n",
      "+--------+--------------------------------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- props: string (nullable = true)\n",
      "\n",
      "+--------+--------------------------------+-----------------------------+\n",
      "|name    |props                           |propsMap                     |\n",
      "+--------+--------------------------------+-----------------------------+\n",
      "|Ashutosh|{\"hair\": \"black\",\"eye\": \"brown\"}|{hair -> black, eye -> brown}|\n",
      "+--------+--------------------------------+-----------------------------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- props: string (nullable = true)\n",
      " |-- propsMap: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, props: string, propsMap: map<string,string>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------------+-----------------------------+-----+\n",
      "|name    |props                           |propsMap                     |eye  |\n",
      "+--------+--------------------------------+-----------------------------+-----+\n",
      "|Ashutosh|{\"hair\": \"black\",\"eye\": \"brown\"}|{hair -> black, eye -> brown}|brown|\n",
      "+--------+--------------------------------+-----------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "data = [('Ashutosh', '{\"hair\": \"black\",\"eye\": \"brown\"}')]\n",
    "\n",
    "schema = ['name', 'props']\n",
    "\n",
    "df = spark.createDataFrame (data, schema)\n",
    "\n",
    "df.show(truncate=False)\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "\n",
    "#propsMap column with MapType gets generates from json sting\n",
    "df1 = df.withColumn('propsMap', F.from_json(df.props, T.MapType(T.StringType(), T.StringType())))\n",
    "\n",
    "df1.show(truncate=False)\n",
    "df1.printSchema()\n",
    "display (df1)\n",
    "\n",
    "#accessing 'eye' key from MapType column 'propsMap'\n",
    "df2= df1.withColumn('eye', df1.propsMap.eye)\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qqyXe1RajkrV",
    "outputId": "6c1adcbf-0aca-4567-99fe-cabc8201b1c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------------+\n",
      "|name    |props                           |\n",
      "+--------+--------------------------------+\n",
      "|Ashutosh|{\"hair\": \"black\",\"eye\": \"brown\"}|\n",
      "+--------+--------------------------------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- props: string (nullable = true)\n",
      "\n",
      "+--------+--------------------------------+--------------+\n",
      "|name    |props                           |propsMap      |\n",
      "+--------+--------------------------------+--------------+\n",
      "|Ashutosh|{\"hair\": \"black\",\"eye\": \"brown\"}|{black, brown}|\n",
      "+--------+--------------------------------+--------------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- props: string (nullable = true)\n",
      " |-- propsMap: struct (nullable = true)\n",
      " |    |-- hair: string (nullable = true)\n",
      " |    |-- eye: string (nullable = true)\n",
      "\n",
      "+--------+--------------------------------+--------------+-----+\n",
      "|name    |props                           |propsMap      |eye  |\n",
      "+--------+--------------------------------+--------------+-----+\n",
      "|Ashutosh|{\"hair\": \"black\",\"eye\": \"brown\"}|{black, brown}|brown|\n",
      "+--------+--------------------------------+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# It's used to convert json string in to MapType or StructType. In this video we discuss about converting it to MapType.\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "data = [('Ashutosh', '{\"hair\": \"black\",\"eye\": \"brown\"}')]\n",
    "\n",
    "schema = ['name', 'props']\n",
    "\n",
    "df = spark.createDataFrame (data, schema)\n",
    "\n",
    "df.show(truncate=False)\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "\n",
    "structSchema = T.StructType([T.StructField('hair', T.StringType()), T.StructField('eye', T.StringType())])\n",
    "\n",
    "df1 = df.withColumn('propsMap', F.from_json(df.props, structSchema))\n",
    "\n",
    "df1.show(truncate=False)\n",
    "df1.printSchema()\n",
    "\n",
    "#accessing 'eye' key from MapType column 'propsMap'\n",
    "df2= df1.withColumn('eye', df1.propsMap.eye)\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMLqhgUGkbqC"
   },
   "source": [
    "### to_json() function\n",
    "\n",
    "*   to_json() is used to convert DataFrame column MapType or Struct Type to JSON string.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yw9mkW-Ykp2x",
    "outputId": "db51eb4f-5827-443e-8a4d-7b2b9e24840e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------------+\n",
      "|name    |properties                   |\n",
      "+--------+-----------------------------+\n",
      "|Ashutosh|{eye -> brown, hair -> black}|\n",
      "+--------+-----------------------------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+--------+-----------------------------+------------------------------+\n",
      "|name    |properties                   |prop                          |\n",
      "+--------+-----------------------------+------------------------------+\n",
      "|Ashutosh|{eye -> brown, hair -> black}|{\"eye\":\"brown\",\"hair\":\"black\"}|\n",
      "+--------+-----------------------------+------------------------------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- prop: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "data = [('Ashutosh', {'hair': 'black', 'eye': 'brown'})]\n",
    "\n",
    "schema = ['name', 'properties']\n",
    "\n",
    "df = spark.createDataFrame (data, schema)\n",
    "df.show(truncate=False)\n",
    "df.printSchema()\n",
    "\n",
    "#here 'prop' column will get generate as json string\n",
    "df1 = df.withColumn('prop', F.to_json(df.properties))\n",
    "\n",
    "df1.show(truncate=False)\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcxiijNtlbmf"
   },
   "source": [
    "### json_tuple() function\n",
    "\n",
    "*   json_tuple() function is used to query or extract elements from json string column and create as new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UVL2jqZJl9t_",
    "outputId": "c1e1c243-a942-4e64-866c-a330c9312c22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------------------------------------+\n",
      "|name    |props                                            |\n",
      "+--------+-------------------------------------------------+\n",
      "|Ashutosh|{\"hair\":\"black\", \"eye\": \"brown\", \"skin\": \"brown\"}|\n",
      "|Bhushan |{\"hair\":\"black\", \"eye\":\"blue\", \"skin\": \"white\"}  |\n",
      "+--------+-------------------------------------------------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- props: string (nullable = true)\n",
      "\n",
      "+--------+-----+-----+\n",
      "|    name| hair| skin|\n",
      "+--------+-----+-----+\n",
      "|Ashutosh|black|brown|\n",
      "| Bhushan|black|white|\n",
      "+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "data = [('Ashutosh', '{\"hair\":\"black\", \"eye\": \"brown\", \"skin\": \"brown\"}'), ('Bhushan', '{\"hair\":\"black\", \"eye\":\"blue\", \"skin\": \"white\"}')]\n",
    "schema = ['name', 'props']\n",
    "\n",
    "df = spark.createDataFrame (data, schema)\n",
    "df.show(truncate=False)\n",
    "df.printSchema()\n",
    "\n",
    "df2 = df.select(df.name, F.json_tuple (df.props, 'hair', 'skin').alias('hair','skin'))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jo0w1BTVm__0"
   },
   "source": [
    "### get_ json_object() function\n",
    "\n",
    "*   It's used to extract the JSON string based on path from the JSON column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VGmodMLKnIsc",
    "outputId": "1abb0769-4c02-47a7-b714-b2dc8831487b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------------------------------------------------+\n",
      "|name    |props                                                             |\n",
      "+--------+------------------------------------------------------------------+\n",
      "|Ashutosh|{\"address\":{\"city\": \"hyd\", \"state\": \"telengana\"},\"gender\": \"male\"}|\n",
      "|Keshav  |{\"address\":{\"city\": \"banglore\", \"state\":\"karnataka\"},\"eye\":\"blue\"}|\n",
      "+--------+------------------------------------------------------------------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- props: string (nullable = true)\n",
      "\n",
      "+--------+--------+---------+\n",
      "|name    |city    |state    |\n",
      "+--------+--------+---------+\n",
      "|Ashutosh|hyd     |telengana|\n",
      "|Keshav  |banglore|karnataka|\n",
      "+--------+--------+---------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "data = [('Ashutosh', '{\"address\":{\"city\": \"hyd\", \"state\": \"telengana\"},\"gender\": \"male\"}'), ('Keshav', '{\"address\":{\"city\": \"banglore\", \"state\":\"karnataka\"},\"eye\":\"blue\"}')]\n",
    "\n",
    "schema = ['name', 'props']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show(truncate=False)\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "df1 = df.select('name', F.get_json_object('props', '$.address.city').alias('city'),\n",
    "                        F.get_json_object('props', '$.address.state').alias('state'))\n",
    "\n",
    "df1.show(truncate=False)\n",
    "df1.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_C1T-ImpORf"
   },
   "source": [
    "### Date functions\n",
    "\n",
    "*   DateType default fomart is yyyy-MM-dd\n",
    "\n",
    "*   current_date() get the current system date. By default, the data will be returned in yyyy-dd-mm format.\n",
    "\n",
    "*   date_format() to parses the date and converts from yyyy-MM-dd to specified format.\n",
    "\n",
    "*   to_date() converts date string in to datetype. We need to specify format of date in the string in the function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WQioHhiVpP8u",
    "outputId": "51ada9d5-cbc7-47d4-859a-5e0db2012238"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|currentDate|\n",
      "+---+-----------+\n",
      "|  0| 2024-02-14|\n",
      "|  1| 2024-02-14|\n",
      "+---+-----------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- currentDate: date (nullable = false)\n",
      "\n",
      "+---+-----------+\n",
      "| id|currentDate|\n",
      "+---+-----------+\n",
      "|  0| 2024.02.14|\n",
      "|  1| 2024.02.14|\n",
      "+---+-----------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- currentDate: string (nullable = false)\n",
      "\n",
      "+---+-----------+\n",
      "| id|currentDate|\n",
      "+---+-----------+\n",
      "|  0| 2024-02-14|\n",
      "|  1| 2024-02-14|\n",
      "+---+-----------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- currentDate: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark.range(2)\n",
    "\n",
    "#datetype default format yyyy-MM-dd I\n",
    "df1= df.withColumn('currentDate', F.current_date())\n",
    "df1.show()\n",
    "df1.printSchema()\n",
    "\n",
    "\n",
    "df2= df1.withColumn('currentDate', F.date_format (df1.currentDate, 'yyyy.MM.dd') )\n",
    "df2.show()\n",
    "df2.printSchema()\n",
    "\n",
    "\n",
    "df3= df2.withColumn('currentDate', F.to_date (df2.currentDate, 'yyyy.MM.dd'))\n",
    "df3.show()\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chiHgl1_tVyM"
   },
   "source": [
    "### datediff(), months_between(), add_months(), date_add(), month(), year()\n",
    "\n",
    "*   DataType default format is yyyy-MM-dd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MyPaj2jktgua",
    "outputId": "9ee5b264-0af2-4382-d77b-1ea962aa21eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----+\n",
      "|        d1|        d2|diff|\n",
      "+----------+----------+----+\n",
      "|2015-04-08|2015-05-08|  30|\n",
      "+----------+----------+----+\n",
      "\n",
      "+----------+----------+--------------+\n",
      "|        d1|        d2|months Between|\n",
      "+----------+----------+--------------+\n",
      "|2015-04-08|2015-05-08|           1.0|\n",
      "+----------+----------+--------------+\n",
      "\n",
      "+----------+----------+----------+\n",
      "|        d1|        d2|  addmonth|\n",
      "+----------+----------+----------+\n",
      "|2015-04-08|2015-05-08|2015-09-08|\n",
      "+----------+----------+----------+\n",
      "\n",
      "+----------+----------+----------+\n",
      "|        d1|        d2|  submonth|\n",
      "+----------+----------+----------+\n",
      "|2015-04-08|2015-05-08|2015-01-08|\n",
      "+----------+----------+----------+\n",
      "\n",
      "+----------+----------+----------+\n",
      "|        d1|        d2|   addDate|\n",
      "+----------+----------+----------+\n",
      "|2015-04-08|2015-05-08|2015-05-12|\n",
      "+----------+----------+----------+\n",
      "\n",
      "+----------+----------+----------+\n",
      "|        d1|        d2|   subDate|\n",
      "+----------+----------+----------+\n",
      "|2015-04-08|2015-05-08|2015-05-04|\n",
      "+----------+----------+----------+\n",
      "\n",
      "+----------+----------+----+\n",
      "|        d1|        d2|year|\n",
      "+----------+----------+----+\n",
      "|2015-04-08|2015-05-08|2015|\n",
      "+----------+----------+----+\n",
      "\n",
      "+----------+----------+-----+\n",
      "|        d1|        d2|month|\n",
      "+----------+----------+-----+\n",
      "|2015-04-08|2015-05-08|    5|\n",
      "+----------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "df = spark.createDataFrame([('2015-04-08', '2015-05-08')], ['d1', 'd2'])\n",
    "\n",
    "\n",
    "# shows difference between 2 dates in days\n",
    "df.withColumn('diff', F.datediff(df.d2, df.d1)).show() # -->  F.datediff(start_date, end_date)\n",
    "\n",
    "# shows difference between 2 dates in months\n",
    "df.withColumn('months Between', F.months_between (df.d2, df.d1)).show() # -->  F.months_between(start_date, end_date)\n",
    "\n",
    "# will add months on specified date\n",
    "df.withColumn ('addmonth', F.add_months (df.d2,4)).show() # --> F.add_months (date, number_of_months_to_be_added)\n",
    "\n",
    "# will subtract months on specified date\n",
    "df.withColumn ('submonth', F.add_months (df.d2,-4)).show()\n",
    "\n",
    "# will add days\n",
    "df.withColumn ('addDate', F.date_add (df.d2,4)).show()\n",
    "\n",
    "# will subtract days\n",
    "df.withColumn('subDate', F.date_add(df.d2,-4)).show()\n",
    "\n",
    "# will give year from the date\n",
    "df.withColumn ('year', F.year(df.d2)).show()\n",
    "\n",
    "# will give months from the date\n",
    "df.withColumn ('month', F.month(df.d2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tytU9nKDv-dm"
   },
   "source": [
    "### Timestamp Functions\n",
    "\n",
    "*   TimestampType default fomart is yyyy-MM-dd HH:mm:ss.SS\n",
    "\n",
    "*   current_timestamp() get the current timestamp. By default, the data will be returne in default format.\n",
    "\n",
    "*   to_timestamp() converts timestamp string in to TimestampType. We need to specify format of timestamp in the string in the function.\n",
    "\n",
    "*   hour(), minute(), second() functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1jnbAxa4xM-b",
    "outputId": "957fbb19-139d-43f6-9922-b0513f5d446f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------+\n",
      "|id |currentTimeStamp          |\n",
      "+---+--------------------------+\n",
      "|0  |2024-02-14 22:41:44.593129|\n",
      "|1  |2024-02-14 22:41:44.593129|\n",
      "+---+--------------------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- currentTimeStamp: timestamp (nullable = false)\n",
      "\n",
      "+---+--------------------------+-------------------+\n",
      "|id |currentTimeStamp          |timestampInSting   |\n",
      "+---+--------------------------+-------------------+\n",
      "|0  |2024-02-14 22:41:44.819816|12.25.2022 08.10.03|\n",
      "|1  |2024-02-14 22:41:44.819816|12.25.2022 08.10.03|\n",
      "+---+--------------------------+-------------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- currentTimeStamp: timestamp (nullable = false)\n",
      " |-- timestampInSting: string (nullable = false)\n",
      "\n",
      "+---+--------------------------+-------------------+\n",
      "|id |currentTimeStamp          |timestampInSting   |\n",
      "+---+--------------------------+-------------------+\n",
      "|0  |2024-02-14 22:41:45.261703|2022-12-25 08:10:03|\n",
      "|1  |2024-02-14 22:41:45.261703|2022-12-25 08:10:03|\n",
      "+---+--------------------------+-------------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- currentTimeStamp: timestamp (nullable = false)\n",
      " |-- timestampInSting: timestamp (nullable = true)\n",
      "\n",
      "+---+--------------------------+----+------+-------+\n",
      "|id |currentTimeStamp          |hour|minute|seconds|\n",
      "+---+--------------------------+----+------+-------+\n",
      "|0  |2024-02-14 22:41:45.526608|22  |41    |45     |\n",
      "|1  |2024-02-14 22:41:45.526608|22  |41    |45     |\n",
      "+---+--------------------------+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "df = spark.range(2)\n",
    "df1 = df.withColumn('currentTimeStamp', F.current_timestamp())\n",
    "df1.show(truncate=False)\n",
    "df1.printSchema()\n",
    "\n",
    "df2= df1.withColumn('timestampInSting', F.lit('12.25.2022 08.10.03'))\n",
    "df2.show(truncate=False)\n",
    "df2.printSchema()\n",
    "\n",
    "df3 = df2.withColumn('timestampInSting', F.to_timestamp(df2.timestampInSting, 'MM.dd.yyyy HH.mm.ss'))\n",
    "df3.show(truncate=False)\n",
    "df3.printSchema()\n",
    "df1.select('*', F.hour (df1.currentTimeStamp).alias('hour'), F.minute(df1.currentTimeStamp).alias('minute'), F.second (df1.currentTimeStamp).alias('seconds')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OW1jNmH95oCt"
   },
   "source": [
    "### Aggregate functions\n",
    "\n",
    "* Aggregate functions operate on a group of rows and calculate a single return\n",
    "value for every group.\n",
    "\n",
    "* approx_count_distinct() – returns the count of distinct items in a group of\n",
    "rows\n",
    "\n",
    "* avg() – returns average of values in a group of rows\n",
    "\n",
    "* collect_list() - returns all values from input column as list with duplicates * collect_set() - returns all values from input column as list without\n",
    "duplicates.\n",
    "\n",
    "* count Distinct() - returns number of distinct elements in input column.\n",
    "* count() - returns number of elements in a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9pmn4CzG56xU",
    "outputId": "0d1ecd52-d86a-4479-83f8-804925b91734"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|     Ashutosh|        HR|  1500|\n",
      "|       Keshav|        IT|  3000|\n",
      "|      Bhushan|        HR|  1500|\n",
      "+-------------+----------+------+\n",
      "\n",
      "+-----------------------------+\n",
      "|approx_count_distinct(salary)|\n",
      "+-----------------------------+\n",
      "|                            2|\n",
      "+-----------------------------+\n",
      "\n",
      "+-----------+\n",
      "|avg(salary)|\n",
      "+-----------+\n",
      "|     2000.0|\n",
      "+-----------+\n",
      "\n",
      "+--------------------+\n",
      "|collect_list(salary)|\n",
      "+--------------------+\n",
      "|  [1500, 3000, 1500]|\n",
      "+--------------------+\n",
      "\n",
      "+-------------------+\n",
      "|collect_set(salary)|\n",
      "+-------------------+\n",
      "|       [3000, 1500]|\n",
      "+-------------------+\n",
      "\n",
      "+----------------------+\n",
      "|count(DISTINCT salary)|\n",
      "+----------------------+\n",
      "|                     2|\n",
      "+----------------------+\n",
      "\n",
      "+-------------+\n",
      "|count(salary)|\n",
      "+-------------+\n",
      "|            3|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "simpleData = [(\"Ashutosh\", \"HR\", 1500),(\"Keshav\", \"IT\", 3000),(\"Bhushan\", \"HR\", 1500)]\n",
    "schema = [\"employee_name\", \"department\", \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame (data = simpleData, schema = schema)\n",
    "df.show()\n",
    "\n",
    "df.select(F.approx_count_distinct('salary')).show()\n",
    "\n",
    "df.select(F.avg('salary')).show()\n",
    "\n",
    "df.select(F.collect_list('salary')).show()\n",
    "\n",
    "df.select(F.collect_set('salary')).show()\n",
    "\n",
    "df.select(F.countDistinct('salary')).show()\n",
    "\n",
    "df.select(F.count('salary')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WoTq3iEp7uxw"
   },
   "source": [
    "### row_number(), rank(), dense_rank()\n",
    "\n",
    "we need to partition the data using Window. partitionBy(), and for row number and rank function we need to additionally order by on partition data using orderBy clause.\n",
    "\n",
    "*  row_number() window function is used to give the sequential row number starting from 1 to the result of each window partition\n",
    "\n",
    "*  rank() window function is used to provide a rank to the result within alie\n",
    "window partition. This function leaves gaps in rank when there are ties.\n",
    "\n",
    "*  dense_rank() window function is used to get the result with rank of\n",
    "rows within a window partition without any gaps. This is similar to rank() function difference being rank function leaves gaps in rank when there are ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FrNs3nKB6qbx",
    "outputId": "5c8fd7d6-c216-419a-d6cd-eb4996ef76fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------+\n",
      "|    name|    dep|salary|\n",
      "+--------+-------+------+\n",
      "|Ashutosh|     HR|  2000|\n",
      "|  Keshav|     IT|  3000|\n",
      "| Bhushan|     HR|  1500|\n",
      "|    Jiya|payroll|  3500|\n",
      "|  shakti|     IT|  3000|\n",
      "|  Varsha|     IT|  4000|\n",
      "|    Ajay|payroll|  2000|\n",
      "|Himanshu|     IT|  2000|\n",
      "|   Akash|     HR|  2000|\n",
      "|    Alex|     IT|  2500|\n",
      "+--------+-------+------+\n",
      "\n",
      "+--------+-------+------+---------+\n",
      "|    name|    dep|salary|rowNumber|\n",
      "+--------+-------+------+---------+\n",
      "| Bhushan|     HR|  1500|        1|\n",
      "|Ashutosh|     HR|  2000|        2|\n",
      "|   Akash|     HR|  2000|        3|\n",
      "|Himanshu|     IT|  2000|        1|\n",
      "|    Alex|     IT|  2500|        2|\n",
      "|  Keshav|     IT|  3000|        3|\n",
      "|  shakti|     IT|  3000|        4|\n",
      "|  Varsha|     IT|  4000|        5|\n",
      "|    Ajay|payroll|  2000|        1|\n",
      "|    Jiya|payroll|  3500|        2|\n",
      "+--------+-------+------+---------+\n",
      "\n",
      "+--------+-------+------+----+\n",
      "|    name|    dep|salary|rank|\n",
      "+--------+-------+------+----+\n",
      "| Bhushan|     HR|  1500|   1|\n",
      "|Ashutosh|     HR|  2000|   2|\n",
      "|   Akash|     HR|  2000|   2|\n",
      "|Himanshu|     IT|  2000|   1|\n",
      "|    Alex|     IT|  2500|   2|\n",
      "|  Keshav|     IT|  3000|   3|\n",
      "|  shakti|     IT|  3000|   3|\n",
      "|  Varsha|     IT|  4000|   5|\n",
      "|    Ajay|payroll|  2000|   1|\n",
      "|    Jiya|payroll|  3500|   2|\n",
      "+--------+-------+------+----+\n",
      "\n",
      "+--------+-------+------+---------+\n",
      "|    name|    dep|salary|denserank|\n",
      "+--------+-------+------+---------+\n",
      "| Bhushan|     HR|  1500|        1|\n",
      "|Ashutosh|     HR|  2000|        2|\n",
      "|   Akash|     HR|  2000|        2|\n",
      "|Himanshu|     IT|  2000|        1|\n",
      "|    Alex|     IT|  2500|        2|\n",
      "|  Keshav|     IT|  3000|        3|\n",
      "|  shakti|     IT|  3000|        3|\n",
      "|  Varsha|     IT|  4000|        4|\n",
      "|    Ajay|payroll|  2000|        1|\n",
      "|    Jiya|payroll|  3500|        2|\n",
      "+--------+-------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "data = [\n",
    "    ('Ashutosh', 'HR', 2000), ('Keshav', 'IT',3000), ('Bhushan', 'HR',1500), ('Jiya', 'payroll',3500),\n",
    "    ('shakti', 'IT', 3000), ('Varsha', 'IT',4000), ('Ajay', 'payroll', 2000), ('Himanshu', 'IT', 2000),\n",
    "    ('Akash', 'HR', 2000), ('Alex', 'IT', 2500)\n",
    "    ]\n",
    "schema = ['name', 'dep', 'salary']\n",
    "\n",
    "df = spark.createDataFrame (data, schema)\n",
    "df.show()\n",
    "\n",
    "window = Window.partitionBy(\"dep\").orderBy('salary')\n",
    "\n",
    "df.withColumn('rowNumber', F.row_number().over (window)).show()\n",
    "\n",
    "df.withColumn('rank', F.rank().over (window)).show()\n",
    "\n",
    "df.withColumn('denserank', F.dense_rank().over (window)).show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "E9F4CWKBiNRn",
    "xQghUJIXZpHc",
    "-MB_Ap1AewTx",
    "pNJ-iBFsD0no",
    "2f9vVE9hdAF1",
    "mYtjcikQi6Eg",
    "SZ1HnVU4NK1F",
    "WOwLOf3DTxof",
    "TaaDm2_VbwD1",
    "bw1pe4L3FCfu",
    "r3zhPR1vRGer"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
